{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # %env CUDA_VISIBLE_DEVICES=0\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from monai.data import Dataset, DataLoader, decollate_batch\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    DivisiblePadd,\n",
        "    Resize, Resized,\n",
        "    ScaleIntensityd,\n",
        "    RandFlipd,\n",
        "    Activations,\n",
        "    AsDiscrete\n",
        ")\n",
        "from monai.networks.nets import UNet, VNet, SegResNet, AttentionUnet, UNETR, SwinUNETR\n",
        "from monai.losses import DiceLoss\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.utils import first, set_determinism\n",
        "from monai.visualize.utils import blend_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_required_divisibility(model_name, **divisibility_kwargs):\n",
        "    if model_name in [\"UNet\", \"AttentionUnet\"]:\n",
        "        # UNet and AttentionUNet: divisibility = product of all strides\n",
        "        # Example: strides=(2,2,2) → 2×2×2 = 8\n",
        "        strides = divisibility_kwargs.get('strides', (2, 2, 2))\n",
        "        total_downsampling = 1\n",
        "        for stride in strides:\n",
        "            total_downsampling *= stride\n",
        "        return total_downsampling\n",
        "    elif model_name == \"VNet\":\n",
        "        # VNet: fixed 5-level architecture\n",
        "        # Architecture: in → down_tr32 → down_tr64 → down_tr128 → down_tr256\n",
        "        # Each transition has stride 2: 2^5 = 32\n",
        "        return 32\n",
        "    elif model_name == \"SegResNet\":\n",
        "        # SegResNet: uses residual blocks with flexible architecture\n",
        "        # Can technically handle various sizes but 16 minimizes edge effects\n",
        "        return 16 # Recommended divisibility for optimal performance\n",
        "    elif model_name == \"UNETR\":\n",
        "        # UNETR: ViT encoder uses fixed 16×16×16 patches (not configurable)\n",
        "        # Input dimensions must be divisible by 16\n",
        "        return 16\n",
        "    elif model_name == \"SwinUNETR\":\n",
        "        # SwinUNETR: depends on spatial_dims and depths\n",
        "        # Downsampling: patch_size × 2^(num_stages-1)\n",
        "        # With defaults: patch_size=2, depths=(2,2,2,2) with 4 stages, so 2 × 2^3 = 16\n",
        "        depths = divisibility_kwargs.get('depths', (2, 2, 2, 2))\n",
        "        num_stages = len(depths)\n",
        "        return 2 ** num_stages  # 2^4 = 16 with default depths\n",
        "    else:\n",
        "        print(f\"Unsupported model name: {model_name}\")\n",
        "        return None\n",
        "\n",
        "def load_data(data_dir, batch_size, resize_dim=None, test_size=0.2, inference=False, model_name=None, **model_kwargs):\n",
        "    if not inference: # Training/Validation\n",
        "        # Define training transforms with data augmentation\n",
        "        train_transforms = [\n",
        "            LoadImaged(keys=[\"brain\", \"lesion\"], image_only=True),\n",
        "            EnsureChannelFirstd(keys=[\"brain\", \"lesion\"]),\n",
        "            ScaleIntensityd(keys=\"brain\"),\n",
        "            RandFlipd(keys=[\"brain\", \"lesion\"], spatial_axis=0, prob=0.5),\n",
        "            RandFlipd(keys=[\"brain\", \"lesion\"], spatial_axis=1, prob=0.5), \n",
        "            RandFlipd(keys=[\"brain\", \"lesion\"], spatial_axis=2, prob=0.5)\n",
        "        ]\n",
        "        # Define validation transforms without augmentation\n",
        "        val_transforms = [\n",
        "            LoadImaged(keys=[\"brain\", \"lesion\"], image_only=True),\n",
        "            EnsureChannelFirstd(keys=[\"brain\", \"lesion\"]),\n",
        "            ScaleIntensityd(keys=\"brain\")\n",
        "        ]\n",
        "        # Add padding or resizing before normalization (ScaleIntensityd)\n",
        "        if resize_dim is None:\n",
        "            required_k = calculate_required_divisibility(model_name, **model_kwargs)\n",
        "            # Padding approach: ensure dimensions are divisible by model requirements\n",
        "            if required_k is not None and required_k > 1:\n",
        "                trainval_pad_transform = DivisiblePadd(keys=[\"brain\", \"lesion\"], k=required_k, method=\"end\")\n",
        "                train_transforms.insert(2, trainval_pad_transform)\n",
        "                val_transforms.insert(2, trainval_pad_transform)\n",
        "        else:\n",
        "            # Resizing approach: resize to fixed dimensions\n",
        "            trainval_resize_transform = Resized(keys=[\"brain\", \"lesion\"], spatial_size=resize_dim, mode=[\"trilinear\", \"nearest\"])\n",
        "            train_transforms.insert(2, trainval_resize_transform)\n",
        "            val_transforms.insert(2, trainval_resize_transform)\n",
        "        train_transforms = Compose(train_transforms)\n",
        "        val_transforms = Compose(val_transforms)\n",
        "        # Load file paths\n",
        "        brains = sorted(glob.glob(os.path.join(data_dir, \"Brain\", \"*.nii.gz\")))\n",
        "        lesions = sorted(glob.glob(os.path.join(data_dir, \"Lesion\", \"*.nii.gz\")))\n",
        "        data_dicts = [\n",
        "            {\"brain\": brain_name, \"lesion\": lesion_name}\n",
        "            for brain_name, lesion_name in zip(brains, lesions)\n",
        "        ]\n",
        "        # Split data into train and validation sets\n",
        "        train_files, val_files = train_test_split(data_dicts, test_size=test_size, random_state=42)\n",
        "        train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "        val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        return train_loader, val_loader\n",
        "    else: # Test\n",
        "        # Custom transform to save original metadata before any spatial transformations\n",
        "        class SaveOriginalMetadata:\n",
        "            def __call__(self, data):\n",
        "                brain = data[\"brain\"]\n",
        "                # Save metadata for later restoration\n",
        "                data[\"original_dim\"] = tuple(brain.shape[1:])\n",
        "                data[\"affine\"] = brain.meta[\"affine\"]\n",
        "                data[\"filename\"] = os.path.basename(brain.meta[\"filename_or_obj\"])\n",
        "                return data\n",
        "        # Build test transforms: save original size, then apply same preprocessing as training\n",
        "        test_transforms = [\n",
        "            LoadImaged(keys=\"brain\", image_only=False), # Load with metadata\n",
        "            EnsureChannelFirstd(keys=\"brain\"),\n",
        "            SaveOriginalMetadata(), # Save original spatial dimensions\n",
        "            ScaleIntensityd(keys=\"brain\")\n",
        "        ]\n",
        "        # Add same padding/resizing as training\n",
        "        if resize_dim is None:\n",
        "            required_k = calculate_required_divisibility(model_name, **model_kwargs)\n",
        "            if required_k is not None and required_k > 1:\n",
        "                test_pad_transform = DivisiblePadd(keys=\"brain\", k=required_k, method=\"end\")\n",
        "                test_transforms.insert(3, test_pad_transform)\n",
        "        else:\n",
        "            test_transforms.insert(3, Resized(keys=\"brain\", spatial_size=resize_dim, mode=\"trilinear\"))\n",
        "        test_transforms = Compose(test_transforms)\n",
        "        brains = sorted(glob.glob(os.path.join(data_dir, \"Brain\", \"*.nii.gz\")))\n",
        "        data_dicts = [{\"brain\": brain_name} for brain_name in brains]\n",
        "        test_ds = Dataset(data=data_dicts, transform=test_transforms)\n",
        "        # Custom collate to preserve metadata as lists instead of tensors\n",
        "        def custom_collate(batch):\n",
        "            return {\n",
        "                'brain': torch.stack([item['brain'] for item in batch]),\n",
        "                'affine': [item['affine'] for item in batch], # Keep as list\n",
        "                'original_dim': [item['original_dim'] for item in batch], # Keep as list\n",
        "                'filename': [item['filename'] for item in batch] # Keep as list\n",
        "            }\n",
        "        test_loader = DataLoader(test_ds, batch_size=1, num_workers=0, pin_memory=torch.cuda.is_available(), collate_fn=custom_collate)\n",
        "        return test_loader\n",
        "\n",
        "def get_model(model_name, img_size, num_classes=1):\n",
        "    if model_name == \"UNet\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Required: spatial dimensions\n",
        "            'in_channels': 1, # Required: input channels\n",
        "            'out_channels': num_classes, # Required: output classes\n",
        "            'channels': (32, 64, 128, 256), # Required: sequence of feature channels\n",
        "            'strides': (2, 2, 2) # Required: sequence of convolution strides\n",
        "        }\n",
        "        model = UNet(**params)\n",
        "    elif model_name == \"VNet\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Default: spatial dimensions\n",
        "            'in_channels': 1, # Default: input channels\n",
        "            'out_channels': num_classes # Non-default: output classes\n",
        "        }\n",
        "        model = VNet(**params)\n",
        "    elif model_name == \"SegResNet\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Default: spatial dimensions\n",
        "            'in_channels': 1, # Default: input channels\n",
        "            'out_channels': num_classes # Non-default: output classes\n",
        "        }\n",
        "        model = SegResNet(**params)\n",
        "    elif model_name == \"AttentionUnet\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Required: spatial dimensions\n",
        "            'in_channels': 1, # Required: input channels\n",
        "            'out_channels': num_classes, # Required: output classes\n",
        "            'channels': (32, 64, 128, 256), # Required: sequence of feature channels\n",
        "            'strides': (2, 2, 2) # Required: sequence of convolutoin strides\n",
        "        }\n",
        "        model = AttentionUnet(**params)\n",
        "    elif model_name == \"UNETR\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Default: spatial dimensions\n",
        "            'in_channels': 1, # Required: input channels\n",
        "            'out_channels': num_classes, # Required: output classes\n",
        "            'img_size': img_size, # Required: input image size\n",
        "            'feature_size': 16, # Default: CNN decoder feature channels\n",
        "            'hidden_size': 768, # Default: transformer embedding dimension\n",
        "            'mlp_dim': 3072, # Default: MLP dimension (typically 4 × hidden_size)\n",
        "            'num_heads': 12 # Default: number of attention heads (same across all layers)\n",
        "        }\n",
        "        model = UNETR(**params)\n",
        "    elif model_name == \"SwinUNETR\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Default: spatial dimensions\n",
        "            'in_channels': 1, # Required: input channels\n",
        "            'out_channels': num_classes, # Required: output classes\n",
        "            'patch_size': 2, # Default: spatial patch size for tokenization\n",
        "            'feature_size': 24, # Default: initial transformer embedding dimension\n",
        "            'depths': (2, 2, 2, 2), # Default: sequence of transformer blocks per stage\n",
        "            'num_heads': (3, 6, 12, 24) # Default: sequence if attention heads per stage\n",
        "        }\n",
        "        model = SwinUNETR(**params)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
        "    return model\n",
        "\n",
        "def get_grad_scaler(device):\n",
        "    if device.type != \"cuda\":\n",
        "        return None\n",
        "    try: # Try newest API first (PyTorch 2.0+)\n",
        "        return torch.GradScaler(\"cuda\")\n",
        "    except (AttributeError, TypeError):\n",
        "        try: # Try torch.amp (PyTorch 1.10+)\n",
        "            return torch.amp.GradScaler(\"cuda\")\n",
        "        except (AttributeError, TypeError): # Fall back to old API\n",
        "            return torch.cuda.amp.GradScaler()\n",
        "\n",
        "def get_autocast_context(device, enabled=True):\n",
        "    if not enabled:\n",
        "        from contextlib import nullcontext\n",
        "        return nullcontext()\n",
        "    try:\n",
        "        # Try newest API first (PyTorch 2.0+)\n",
        "        return torch.autocast(device_type=device.type, dtype=torch.float16)\n",
        "    except (AttributeError, TypeError):\n",
        "        try:\n",
        "            # Try torch.amp (PyTorch 1.10+)\n",
        "            return torch.amp.autocast(device.type)\n",
        "        except (AttributeError, TypeError):\n",
        "            # Fall back to CUDA-specific (old)\n",
        "            if device.type == \"cuda\":\n",
        "                return torch.cuda.amp.autocast()\n",
        "            else:\n",
        "                from contextlib import nullcontext\n",
        "                return nullcontext()\n",
        "\n",
        "def train_one_epoch(model, device, train_loader, optimizer, criterion, scaler, metric, post_pred):\n",
        "    model.train() # Set model to training mode\n",
        "    epoch_loss = 0.0\n",
        "    metric.reset()\n",
        "    for batch_data in train_loader:\n",
        "        # Prepare data\n",
        "        images, labels = (\n",
        "            batch_data[\"brain\"].to(device),\n",
        "            batch_data[\"lesion\"].to(device),\n",
        "        )\n",
        "        # Forward pass with mixed precision (if available)\n",
        "        optimizer.zero_grad()\n",
        "        with get_autocast_context(device, enabled=(scaler is not None)):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        # Backward pass with gradient scaling (if available)\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # Accumulate metrics\n",
        "        epoch_loss += loss.item()\n",
        "        outputs = [post_pred(i) for i in decollate_batch(outputs)] # Apply post-processing\n",
        "        metric(y_pred=outputs, y=labels)\n",
        "    epoch_metric = metric.aggregate().item()\n",
        "    return epoch_loss / len(train_loader), epoch_metric\n",
        "\n",
        "def validate_one_epoch(model, device, val_loader, metric, post_pred):\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for batch_data in val_loader:\n",
        "            images, labels = (\n",
        "                batch_data[\"brain\"].to(device),\n",
        "                batch_data[\"lesion\"].to(device),\n",
        "            )\n",
        "            outputs = model(images)\n",
        "            outputs = [post_pred(i) for i in decollate_batch(outputs)]\n",
        "            metric(y_pred=outputs, y=labels)\n",
        "    return metric.aggregate().item()\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=30, delta=0):\n",
        "        self.patience = patience # Number of epochs to wait before stopping\n",
        "        self.delta = delta # Minimum improvement threshold\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "    def __call__(self, metric):\n",
        "        score = metric\n",
        "        if self.best_score is None: # First epoch\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score - self.delta: # Metric decreased\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else: # Metric improved\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "\n",
        "def train_model(model_dir, model, device, train_loader, val_loader, logger,\n",
        "        criterion, metric, post_pred, max_epochs=100, learning_rate=1e-4, weight_decay=1e-5, val_interval=1, es_patience=30):\n",
        "    # Setup optimizer and learning rate scheduler\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
        "    scaler = get_grad_scaler(device)\n",
        "    start_time = time.time()\n",
        "    best_metric = -1\n",
        "    best_metric_epoch = -1\n",
        "    best_model_state = None\n",
        "    early_stopping = EarlyStopping(patience=es_patience, delta=0)\n",
        "    epoch_loss_values, epoch_metric_values, metric_values = [], [], []\n",
        "    for epoch in range(max_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        # Training phase\n",
        "        epoch_loss, epoch_metric = train_one_epoch(model, device, train_loader, optimizer, criterion, scaler, metric, post_pred)\n",
        "        epoch_loss_values.append(epoch_loss)\n",
        "        epoch_metric_values.append(epoch_metric)\n",
        "        # Validation phase\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            val_metric = validate_one_epoch(model, device, val_loader, metric, post_pred)\n",
        "            metric_values.append(val_metric)\n",
        "            # Save best model\n",
        "            if val_metric > best_metric:\n",
        "                best_metric = val_metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                best_model_state = model.state_dict()\n",
        "                torch.save(model.state_dict(), os.path.join(model_dir, \"BestMetricModel.pth\"))\n",
        "                logger.info(f\"Best DSC: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
        "            # Check early stopping\n",
        "            early_stopping(val_metric)\n",
        "            if early_stopping.early_stop:\n",
        "                logger.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "                print(f\"; Early stopping triggered at epoch {epoch + 1}\", end=\"\")\n",
        "                break\n",
        "        epoch_end_time = time.time()\n",
        "        logger.info(\n",
        "            f\"Epoch {epoch + 1} completed for {(epoch_end_time - epoch_start_time)/60:.2f} mins - \"\n",
        "            f\"Training loss: {epoch_loss:.4f}, Training DSC: {epoch_metric:.4f}, Validation DSC: {val_metric:.4f}\"\n",
        "        )\n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "        sys.stdout.write(f\"\\rEpoch {epoch + 1}/{max_epochs} completed\")\n",
        "        sys.stdout.flush()\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    logger.info(\n",
        "        f\"Best DSC: {best_metric:.3f} at epoch {best_metric_epoch}; \"\n",
        "        f\"Total time consumed: {total_time/60:.2f} mins\"\n",
        "    )\n",
        "    print(\n",
        "        f\"\\nBest DSC: {best_metric:.3f} at epoch {best_metric_epoch}; \"\n",
        "        f\"Total time consumed: {total_time/60:.2f} mins\"\n",
        "    )\n",
        "    # Load best model weights\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    return model, epoch_loss_values, epoch_metric_values, metric_values\n",
        "\n",
        "def plot_metric_values(model_dir, epoch_loss_values, epoch_metric_values, metric_values, val_interval=1):\n",
        "    _, axs = plt.subplots(1, 2, figsize=(8, 5))\n",
        "    axs[0].plot( [i + 1 for i in range(len(epoch_loss_values))], epoch_loss_values, label='Training Loss', color='red')\n",
        "    axs[0].set_title('Training Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[1].plot([i + 1 for i in range(len(epoch_metric_values))], epoch_metric_values, label='Training DSC', color='red')\n",
        "    axs[1].plot([val_interval * (i + 1) for i in range(len(metric_values))], metric_values, label='Validation DSC', color='blue')\n",
        "    axs[1].set_title('Training DSC vs. Validation DSC')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_ylabel('DSC')\n",
        "    axs[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_dir, \"Performance.png\"), dpi=300)\n",
        "\n",
        "class GradCAM3D:\n",
        "    def __init__(self, model, target_layer, criterion):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self.model.eval()\n",
        "        # Register hooks to capture intermediate features and gradients\n",
        "        target_layer.register_forward_hook(self.save_activation)\n",
        "        target_layer.register_full_backward_hook(self.save_gradient)\n",
        "        self.loss = criterion\n",
        "    def save_activation(self, _module, _input, output):\n",
        "        # Capture forward pass activations\n",
        "        self.activations = output.detach()\n",
        "    def save_gradient(self, _module, _grad_input, grad_output):\n",
        "        # Capture backward pass gradients\n",
        "        self.gradients = grad_output[0].detach()\n",
        "    def __call__(self, brain, lesion):\n",
        "        # Forward pass to compute loss\n",
        "        self.model.zero_grad()\n",
        "        output = self.model(brain)\n",
        "        loss = self.loss(output, lesion)\n",
        "        loss.backward()\n",
        "        # Compute importance weights by global average pooling of gradients\n",
        "        gradients = self.gradients\n",
        "        activations = self.activations\n",
        "        weights = torch.mean(gradients, dim=(2, 3, 4), keepdim=True) # Global average pooling\n",
        "        # Weighted combination of activation maps\n",
        "        cam = torch.sum(weights * activations, dim=1, keepdim=True) # Weighted sum\n",
        "        cam = F.relu(cam) # Apply ReLU to focus on positive contributions\n",
        "        # Resize CAM to match input dimensions\n",
        "        cam = F.interpolate(cam, size=brain.shape[2:], mode='trilinear', align_corners=False)\n",
        "        # Normalize to [0, 1] range\n",
        "        cam = cam - torch.min(cam)\n",
        "        cam = cam / torch.max(cam)\n",
        "        return cam.squeeze(0)\n",
        "\n",
        "def apply_gradcam_to_sample(model, val_loader, target_layers, device, criterion, sample_index, slice_index):\n",
        "    # Load sample data\n",
        "    brain = val_loader.dataset[sample_index][\"brain\"].to(device) \n",
        "    lesion = val_loader.dataset[sample_index][\"lesion\"].to(device)\n",
        "    # Setup visualization grid\n",
        "    _, axes = plt.subplots(len(target_layers), 3, figsize=(12, 4 * len(target_layers)))\n",
        "    # Extract 2D slices for visualization\n",
        "    brain_slice = torch.rot90(brain[0, :, :, slice_index], k=1, dims=(0, 1))\n",
        "    lesion_slice = torch.rot90(lesion[0, :, :, slice_index], k=1, dims=(0,1))\n",
        "    # Iterate through each target layer\n",
        "    for i, (layer_name, target_layer) in enumerate(target_layers.items()):\n",
        "        # Initialize GradCAM for current layer\n",
        "        grad_cam = GradCAM3D(model, target_layer, criterion)\n",
        "        # Compute GradCAM heatmap\n",
        "        with torch.no_grad():\n",
        "            _ = model(brain.unsqueeze(0)) # Warm-up forward pass\n",
        "        cam = grad_cam(brain.unsqueeze(0), lesion.unsqueeze(0))\n",
        "        # Extract 2D slice from GradCAM heatmap\n",
        "        cam_slice = torch.rot90(cam[0, :, :, slice_index], k=1, dims=(0,1))\n",
        "        # Visualize: Original image\n",
        "        axes[i, 0].imshow(brain_slice.detach().cpu(), cmap='gray')\n",
        "        axes[i, 0].set_title(\"Brain\")\n",
        "        axes[i, 0].axis('off')\n",
        "        # Visualize: Ground truth lesion\n",
        "        axes[i, 1].imshow(lesion_slice.detach().cpu(), cmap='gray')\n",
        "        axes[i, 1].set_title(\"Lesion\")\n",
        "        axes[i, 1].axis('off')\n",
        "        # Visualize: GradCAM overlay\n",
        "        axes[i, 2].imshow(brain_slice.detach().cpu(), cmap='gray')\n",
        "        axes[i, 2].imshow(cam_slice.detach().cpu(), cmap='jet', alpha=0.5)\n",
        "        axes[i, 2].set_title(f\"GradCAM: {layer_name}\")\n",
        "        axes[i, 2].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def get_gradcam_target_layers(model, model_name):\n",
        "    target_layers = {}\n",
        "    if model_name == \"UNet\":\n",
        "        # UNet structure: model.model contains [encoder, bottleneck, decoder, final_conv]\n",
        "        target_layers = {\n",
        "            \"Encoder Last\": model.model[0][-1],  # Last encoder block - captures high-level semantic features\n",
        "            \"Bottleneck\": model.model[1],  # Bottleneck layer - most compressed feature representation\n",
        "            \"Decoder First\": model.model[2][0],  # First decoder block - begins spatial detail restoration\n",
        "            \"Final Conv\": model.model[-1]  # Final convolution layer - produces segmentation output\n",
        "        }\n",
        "    elif model_name == \"VNet\":\n",
        "        # VNet structure: down_tr (encoder transitions), up_tr (decoder transitions), out_tr (output)\n",
        "        # Note: VNet uses \"transition\" modules (downsampling + residual connection + feature processing) instead of traditional \"blocks\"\n",
        "        target_layers = {\n",
        "            \"Encoder Last\": model.down_tr256,  # Last encoder transition - captures highest level abstraction\n",
        "            \"Decoder First\": model.up_tr256,  # First decoder transition - begins spatial reconstruction\n",
        "            \"Final Conv\": model.out_tr  # Output transition - produces final segmentation output\n",
        "        }\n",
        "    elif model_name == \"SegResNet\":\n",
        "        # SegResNet structure: down_layers (encoder, list of lists), up_layers (decoder, list of lists), conv_final (output, list)\n",
        "        target_layers = {\n",
        "            \"Encoder Last\": model.down_layers[-1][-1],  # Last encoder layer - captures highest level abstraction\n",
        "            \"Decoder First\": model.up_layers[0][0],  # First decoder layer - begins spatial reconstruction\n",
        "            \"Final Conv\": model.conv_final[-1]  # Final convolution layer - produces segmentation output\n",
        "        }\n",
        "    elif model_name == \"AttentionUnet\":\n",
        "        # AttentionUNet structure: encoder, attention gates, decoder, output\n",
        "        try:\n",
        "            target_layers = {\n",
        "                \"Encoder Last\": model.model[0][-1],  # Last encoder block - captures high-level semantic features\n",
        "                \"Decoder First\": model.model[2][0],  # First decoder block - begins spatial detail restoration\n",
        "                \"Final Conv\": model.model[-1]  # Final convolution layer - produces segmentation output\n",
        "            }\n",
        "        except (AttributeError, IndexError):\n",
        "            print(f\"Warning: Could not auto-detect AttentionUnet layers\")\n",
        "    elif model_name == \"UNETR\":\n",
        "        # UNETR structure: vit (transformer encoder), encoder (CNN), decoder (CNN), out (output)\n",
        "        target_layers = {\n",
        "            \"Transformer Last\": model.vit.blocks[-1],  # Last transformer block - captures global context features\n",
        "            \"Encoder Last\": model.encoder4,  # Last CNN encoder - fuses transformer and CNN features\n",
        "            \"Decoder First\": model.decoder5,  # First decoder block - begins spatial reconstruction\n",
        "            \"Final Conv\": model.out  # Final convolution layer - produces segmentation output\n",
        "        }\n",
        "    elif model_name == \"SwinUNETR\":\n",
        "        # SwinUNETR structure: swinViT (Swin transformer), encoder (CNN), decoder (CNN), out (output)\n",
        "        target_layers = {\n",
        "            \"Swin Last\": model.swinViT.layers3[-1].blocks[-1],  # Last Swin block - captures hierarchical attention features\n",
        "            \"Encoder Last\": model.encoder4,  # Last CNN encoder - fuses transformer and CNN features\n",
        "            \"Decoder First\": model.decoder5,  # First decoder block - begins spatial reconstruction\n",
        "            \"Final Conv\": model.out  # Final convolution layer - produces segmentation output\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
        "    return target_layers\n",
        "\n",
        "def apply_best_model(model_dir, model, device, test_loader, post_pred, pred_dir, resize_dim):\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(torch.load(os.path.join(model_dir, \"BestMetricModel.pth\")))\n",
        "    model.eval()\n",
        "    os.makedirs(pred_dir, exist_ok=True)\n",
        "    with torch.no_grad():\n",
        "        for batch_data in test_loader:\n",
        "            brain = batch_data[\"brain\"].to(device)\n",
        "            affine = batch_data[\"affine\"][0] # Affine matrix for NIfTI\n",
        "            original_dim = tuple(batch_data[\"original_dim\"][0]) # Original spatial dimensions\n",
        "            filename = batch_data[\"filename\"][0]\n",
        "            # Model inference\n",
        "            output = model(brain)\n",
        "            output = post_pred(output[0]) # Apply post-processing\n",
        "            current_dim = tuple(output.shape[1:])\n",
        "            # Restore to original dimensions if needed\n",
        "            if current_dim != original_dim:\n",
        "                if resize_dim is not None:\n",
        "                    # If resizing was used: apply inverse resizing\n",
        "                    output = Resize(spatial_size=original_dim, mode=\"nearest\")(output)\n",
        "                    print(f\"{filename}: Resized {current_dim} → {original_dim}\")\n",
        "                else:\n",
        "                    # If padding was used: crop back to original size\n",
        "                    output = output[:, :original_dim[0], :original_dim[1], :original_dim[2]]\n",
        "                    print(f\"{filename}: Cropped {current_dim} → {original_dim}\")\n",
        "            # Save as NIfTI file\n",
        "            nifti_image = nib.Nifti1Image(output.squeeze().detach().cpu().numpy(), affine)\n",
        "            pred_file = os.path.join(pred_dir, filename)\n",
        "            nib.save(nifti_image, pred_file)\n",
        "\n",
        "def calculate_test_metric(metric, gt_dir, pred_dir, pred_prefix=\"\", pred_suffix=\"\"):\n",
        "    metric_values = []\n",
        "    nos = []\n",
        "    filenames = [f for f in os.listdir(gt_dir) if f.endswith('.nii.gz')]\n",
        "    for filename in filenames:\n",
        "        no = filename.replace('.nii.gz', '')\n",
        "        gt_file = os.path.join(gt_dir, filename)\n",
        "        pred_file = os.path.join(pred_dir, f\"{pred_prefix}{no}{pred_suffix}.nii.gz\")\n",
        "        if not os.path.exists(pred_file):\n",
        "            print(f\"Warning: {no} not found. Skipping this sample.\")\n",
        "            continue\n",
        "        # Load ground truth and prediction\n",
        "        gt_data = nib.load(gt_file).get_fdata()\n",
        "        pred_data = nib.load(pred_file).get_fdata()\n",
        "        gt_sum = gt_data.sum()\n",
        "        pred_sum = pred_data.sum()\n",
        "        # Compute metric\n",
        "        output = torch.from_numpy(pred_data).unsqueeze(0).unsqueeze(0)\n",
        "        label = torch.from_numpy(gt_data).unsqueeze(0).unsqueeze(0)\n",
        "        metric_value = metric(y_pred=output, y=label)\n",
        "        # Handle NaN cases\n",
        "        if not torch.isnan(metric_value):\n",
        "            metric_values.append(metric_value.item())\n",
        "            nos.append(no)\n",
        "        else:\n",
        "            print(f\"Warning: Metric calculation for {no} resulted in NaN.\")\n",
        "            print(f\"  GT sum: {gt_sum}, Pred sum: {pred_sum}\")\n",
        "            if gt_sum == 0 and pred_sum == 0:\n",
        "                # Both empty: perfect prediction\n",
        "                print(f\"  → Both GT and Prediction are empty. Setting Dice to 1.0\")\n",
        "                metric_values.append(1.0)\n",
        "                nos.append(no)\n",
        "            else:\n",
        "                print(f\"  → Skipping this sample.\")\n",
        "    if len(metric_values) == 0:\n",
        "        print(\"Error: No valid metrics calculated!\")\n",
        "        return\n",
        "    # Compute statistics\n",
        "    metric_values = np.array(metric_values)\n",
        "    print(f\"\\u2022 mean \\u00B1 standard deviation: {np.mean(metric_values):.3f} \\u00B1 {np.std(metric_values):.3f}\")\n",
        "    print(f\"\\u2022 [minimum, maximum]: [{np.min(metric_values):.3f}, {np.max(metric_values):.3f}]\")\n",
        "    # Save results to CSV\n",
        "    pred_df = pd.DataFrame({\"No\": nos, \"DSC\": [f\"{value:.3f}\" for value in metric_values]})\n",
        "    pred_df.to_csv(os.path.join(pred_dir, \"DSC.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = os.path.join(\"LesionSegmentation\", \"Datasets\")\n",
        "model_dir_prefix = \"LesionSegmentation\"\n",
        "model_name = \"SegResNet\" # any supported model name: UNet, VNet, SegResNet, AttentionUnet, UNETR, SwinUNETR\n",
        "num_classes = 1 # for binary segmentation\n",
        "resize_dim = None # Use padding or specify tuple for resizing\n",
        "test_size = 0.2\n",
        "batch_size = 5\n",
        "max_epochs = 100\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-5\n",
        "val_interval = 1\n",
        "es_patience = 30\n",
        "\n",
        "# Setup output directory and logging\n",
        "model_dir = f\"{model_dir_prefix}_{model_name}\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "log_file = os.path.join(model_dir, \"Prediction.log\")\n",
        "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(message)s\")\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_determinism(seed=0)\n",
        "train_loader, val_loader = load_data(\n",
        "    os.path.join(data_dir, \"train\"), batch_size, resize_dim=resize_dim, test_size=test_size,\n",
        "    inference=False, model_name=model_name \n",
        ")\n",
        "\n",
        "# Check data shape\n",
        "tr = first(train_loader)\n",
        "img_size = tuple(tr[\"brain\"].shape[-3:])\n",
        "print('\\nData shape for training:')\n",
        "for key, value in tr.items():\n",
        "    print(f'\\u2022 {key}: {tuple(value.shape)} \\u00D7 {len(train_loader)}')\n",
        "vl = first(val_loader)\n",
        "print('\\nData shape for validation:')\n",
        "for key, value in vl.items():\n",
        "    print(f'\\u2022 {key}: {tuple(value.shape)} \\u00D7 {len(val_loader)}')\n",
        "\n",
        "# Visualize data\n",
        "sample_index = 19 \n",
        "slice_index = 52\n",
        "_, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
        "brain = train_loader.dataset[sample_index][\"brain\"].detach().cpu() \n",
        "lesion = train_loader.dataset[sample_index][\"lesion\"].detach().cpu()\n",
        "brain_slice = torch.rot90(brain[0, :, :, slice_index], k=1, dims=(0, 1))\n",
        "lesion_slice = torch.rot90(lesion[0, :, :, slice_index], k=1, dims=(0,1))\n",
        "blended = blend_images(brain, lesion, alpha=0.5)\n",
        "blended_slice = torch.rot90(blended[0, :, :, slice_index], k=1, dims=(0,1)).squeeze()\n",
        "axs[0].imshow(brain_slice, cmap='gray')\n",
        "axs[0].set_title(\"Brain\")\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(lesion_slice, cmap='gray')\n",
        "axs[1].set_title(\"Lesion\")\n",
        "axs[1].axis('off')\n",
        "axs[2].imshow(blended_slice, cmap='gray')\n",
        "axs[2].set_title(\"Lesion-overlaied Brain\")\n",
        "axs[2].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_model(model_name, img_size, num_classes=num_classes)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "model.to(device)\n",
        "print(f\"Selected model: {model_name}\")\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_params:,}\")\n",
        "criterion = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, sigmoid=True)\n",
        "metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "post_pred = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
        "model, epoch_loss_values, epoch_metric_values, metric_values = train_model(\n",
        "    model_dir, model, device, train_loader, val_loader, logger,\n",
        "    criterion, metric, post_pred, max_epochs, learning_rate, weight_decay, val_interval\n",
        ")\n",
        "plot_metric_values(model_dir, epoch_loss_values, epoch_metric_values, metric_values, val_interval)\n",
        "\n",
        "# Visualize outcome\n",
        "sample_index = 18\n",
        "slice_index = 45\n",
        "model.eval()\n",
        "metric.reset()\n",
        "with torch.no_grad():\n",
        "    brain = val_loader.dataset[sample_index][\"brain\"].to(device)\n",
        "    lesion = val_loader.dataset[sample_index][\"lesion\"].to(device)\n",
        "    output = model(brain.unsqueeze(0))\n",
        "    output = post_pred(output).squeeze(0)\n",
        "    metric(y_pred=output, y=lesion)\n",
        "metric_value = metric.aggregate().item()\n",
        "_, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
        "brain_slice = torch.rot90(brain[0, :, :, slice_index], k=1, dims=(0, 1))\n",
        "lesion_slice = torch.rot90(lesion[0, :, :, slice_index], k=1, dims=(0,1))\n",
        "output_slice = torch.rot90(output[0, :, :, slice_index], k=1, dims=(0,1))\n",
        "axs[0].imshow(brain_slice.detach().cpu(), cmap=\"gray\")\n",
        "axs[0].set_title(\"Brain\")\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(lesion_slice.detach().cpu(), cmap=\"gray\")\n",
        "axs[1].set_title(\"Lesion\")\n",
        "axs[1].axis('off')\n",
        "axs[2].imshow(output_slice.detach().cpu(), cmap=\"gray\")\n",
        "axs[2].set_title(f\"Predicted Lesion: DSC = {metric_value:.3f}\")\n",
        "axs[2].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GradCAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_index = 18\n",
        "slice_index = 45\n",
        "target_layers = get_gradcam_target_layers(model, model_name)\n",
        "apply_gradcam_to_sample(model, val_loader, target_layers, device, criterion, sample_index, slice_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader = load_data(\n",
        "    os.path.join(data_dir, \"test\"), None, resize_dim=resize_dim, test_size=None,\n",
        "    inference=True, model_name=model_name\n",
        ")\n",
        "pred_dir = os.path.join(model_dir, \"Prediction\")\n",
        "apply_best_model(model_dir, model, device, test_loader, post_pred, pred_dir, resize_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assess Performance\n",
        "#### Research level\n",
        "- Excellent: DSC > 0.60\n",
        "- Good: DSC 0.50-0.60\n",
        "- Acceptable: DSC 0.40-0.50\n",
        "- Poor: DSC < 0.40\n",
        "#### Practical level\n",
        "- Excellent: DSC > 0.50\n",
        "- Good: DSC 0.40-0.50\n",
        "- Acceptable: DSC 0.30-0.40\n",
        "- Poor: DSC < 0.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gt_dir = os.path.join(data_dir, \"test\", \"Lesion\")\n",
        "print('DSC on test set:')\n",
        "calculate_test_metric(metric, gt_dir, pred_dir, pred_prefix=\"\", pred_suffix=\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Anonymous",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
