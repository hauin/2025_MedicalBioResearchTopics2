{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "851ca53b",
      "metadata": {},
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395234c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # %env CUDA_VISIBLE_DEVICES=0\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Subset\n",
        "import time\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from monai.data import Dataset, DataLoader\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    DivisiblePadd,\n",
        "    Resized,\n",
        "    ScaleIntensityd,\n",
        "    RandAdjustContrastd,\n",
        "    RandGaussianNoised\n",
        ")\n",
        "from monai.networks.nets import Regressor, ResNet, DenseNet121, SEResNet50, EfficientNetBN, ViT\n",
        "from monai.metrics import MAEMetric\n",
        "from monai.utils import first, set_determinism\n",
        "import shap\n",
        "from nilearn import plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ada23a3",
      "metadata": {},
      "source": [
        "### Define Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a283c3e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_required_divisibility(model_name, **divisibility_kwargs):\n",
        "    if model_name in [\"Regressor\", \"ResNet50\", \"DenseNet121\", \"SEResNet50\", \"EfficientNetB0\", \"SFCN\"]:\n",
        "        # These models use adaptive pooling - any size works\n",
        "        return None\n",
        "    elif model_name == \"ViT\":\n",
        "        # ViT: input must be divisible by patch_size\n",
        "        patch_size = divisibility_kwargs.get('patch_size', 16)\n",
        "        if isinstance(patch_size, (list, tuple)):\n",
        "            return patch_size[0]\n",
        "        else:\n",
        "            return patch_size\n",
        "    else:\n",
        "        print(f\"Unsupported model name: {model_name}\")\n",
        "        return None\n",
        "\n",
        "def load_data(data_dir, modalities, batch_size, resize_dim=None, test_size=0.2, inference=False, model_name=None, **divisibility_params):\n",
        "    df = pd.read_csv(os.path.join(data_dir, 'Subjects.csv'))\n",
        "    subjects = df['ID'].apply(lambda x: f'{x:03d}').to_numpy()\n",
        "    data_dicts = []\n",
        "    for index, subject in enumerate(subjects):\n",
        "        subject_dict = {}\n",
        "        for modality in modalities:\n",
        "            subject_dict[modality] = os.path.join(data_dir, modality, f\"{subject}.nii.gz\")\n",
        "        if not inference:\n",
        "            subject_dict['Age'] = df['Age'].to_numpy()[index]\n",
        "        data_dicts.append(subject_dict)\n",
        "    if not inference: # Training/Validation\n",
        "        # Define training transforms with minimal intensity-based augmentatioin\n",
        "        train_transforms = [\n",
        "            LoadImaged(keys=modalities, image_only=True),\n",
        "            EnsureChannelFirstd(keys=modalities),\n",
        "            ScaleIntensityd(keys=modalities, minv=0, maxv=1),\n",
        "            RandAdjustContrastd(keys=modalities, prob=0.3, gamma=(0.9, 1.1)),\n",
        "            RandGaussianNoised(keys=modalities, prob=0.3, std=0.01)\n",
        "        ]\n",
        "        # Define validation transforms without augmentation\n",
        "        val_transforms = [\n",
        "            LoadImaged(keys=modalities, image_only=True),\n",
        "            EnsureChannelFirstd(keys=modalities),\n",
        "            ScaleIntensityd(keys=modalities, minv=0, maxv=1)\n",
        "        ]\n",
        "        # Add padding or resizing before normalization (ScaleIntensityd)\n",
        "        if resize_dim is None:\n",
        "            required_k = calculate_required_divisibility(model_name, **divisibility_params)\n",
        "            if required_k is not None and required_k > 1:\n",
        "                trainval_pad_transform = DivisiblePadd(keys=modalities, k=required_k, method=\"symmetric\")\n",
        "                train_transforms.insert(2, trainval_pad_transform)\n",
        "                val_transforms.insert(2, trainval_pad_transform)\n",
        "        else:\n",
        "            trainval_resize_transform = Resized(keys=modalities, spatial_size=resize_dim, mode=\"trilinear\")\n",
        "            train_transforms.insert(2, trainval_resize_transform)\n",
        "            val_transforms.insert(2, trainval_resize_transform)\n",
        "        train_transforms = Compose(train_transforms)\n",
        "        val_transforms = Compose(val_transforms)\n",
        "        # Split data into train and validation sets\n",
        "        train_files, val_files = train_test_split(data_dicts, test_size=test_size, random_state=42)\n",
        "        train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "        val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        return train_loader, val_loader\n",
        "    else: # Test\n",
        "        # Define test transforms\n",
        "        test_transforms = [\n",
        "            LoadImaged(keys=modalities, image_only=True),\n",
        "            EnsureChannelFirstd(keys=modalities),\n",
        "            ScaleIntensityd(keys=modalities, minv=0, maxv=1)\n",
        "        ]\n",
        "        # Add padding or resizing\n",
        "        if resize_dim is None:\n",
        "            required_k = calculate_required_divisibility(model_name, **divisibility_params)\n",
        "            if required_k is not None and required_k > 1:\n",
        "                pad_transform = DivisiblePadd(keys=modalities, k=required_k, method=\"symmetric\")\n",
        "                test_transforms.insert(2, pad_transform)\n",
        "        else:\n",
        "            resize_transform = Resized(keys=modalities, spatial_size=resize_dim, mode=\"trilinear\")\n",
        "            test_transforms.insert(2, resize_transform)\n",
        "        test_transforms = Compose(test_transforms)\n",
        "        test_ds = Dataset(data=data_dicts, transform=test_transforms)\n",
        "        test_loader = DataLoader(test_ds, batch_size=1, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        return test_loader, subjects\n",
        "\n",
        "class SFCN(nn.Module): # https://github.com/ha-ha-ha-han/UKBiobank_deep_pretrain/blob/master/dp_model/model_files/sfcn.py\n",
        "    def __init__(self, input_channels, output_dim, channel_number=[32, 64, 128, 256, 256, 64], feature_dim=64, dropout=True):\n",
        "        super(SFCN, self).__init__()\n",
        "        n_layer = len(channel_number)\n",
        "        self.feature_extractor = nn.Sequential()\n",
        "        # Build convolutional layers\n",
        "        for i in range(n_layer):\n",
        "            in_channel = input_channels if i == 0 else channel_number[i-1]\n",
        "            out_channel = channel_number[i]\n",
        "            if i < n_layer-1:\n",
        "                self.feature_extractor.add_module(\n",
        "                    f'conv_{i}',\n",
        "                    self.conv_layer(in_channel, out_channel, maxpool=True, kernel_size=3, padding=1)\n",
        "                )\n",
        "            else:\n",
        "                self.feature_extractor.add_module(\n",
        "                    f'conv_{i}',\n",
        "                    self.conv_layer(in_channel, out_channel, maxpool=False, kernel_size=1, padding=0)\n",
        "                )\n",
        "        # Pooling and projection\n",
        "        self.global_pool = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool3d((1, 1, 1)),\n",
        "            nn.Dropout(0.5) if dropout else nn.Identity(),\n",
        "            nn.Conv3d(channel_number[-1], feature_dim, kernel_size=1)\n",
        "        )\n",
        "        # Additional FC layers for refinement\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(feature_dim, feature_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(feature_dim, output_dim)\n",
        "        )\n",
        "    @staticmethod\n",
        "    def conv_layer(in_channel, out_channel, maxpool=True, kernel_size=3, padding=0, maxpool_stride=2):\n",
        "        if maxpool:\n",
        "            layer = nn.Sequential(\n",
        "                nn.Conv3d(in_channel, out_channel, padding=padding, kernel_size=kernel_size),\n",
        "                nn.BatchNorm3d(out_channel),\n",
        "                nn.MaxPool3d(2, stride=maxpool_stride),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "        else:\n",
        "            layer = nn.Sequential(\n",
        "                nn.Conv3d(in_channel, out_channel, padding=padding, kernel_size=kernel_size),\n",
        "                nn.BatchNorm3d(out_channel),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        return layer\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.regressor(x)\n",
        "        return x\n",
        "\n",
        "def get_model(model_name, num_input_channels, img_size):\n",
        "    if model_name == \"Regressor\":\n",
        "        params = {\n",
        "            'in_shape': [num_input_channels, *img_size], # Required: input shape\n",
        "            'out_shape': 1, # Required: output dimension\n",
        "            'channels': (16, 32, 64, 128, 256), # Required: sequence of feature channels\n",
        "            'strides': (2, 2, 2, 2) # Required: sequence of convolution strides\n",
        "        }\n",
        "        model = Regressor(**params)\n",
        "    elif model_name == \"ResNet50\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Default: spatial dimensions\n",
        "            'n_input_channels': num_input_channels, # Non-default: input channels\n",
        "            'num_classes': 1, # Non-default: regression output\n",
        "            'block': \"bottleneck\", # Required: bottleneck block for ResNet50\n",
        "            'layers': [3, 4, 6, 3], # Required: layer configuration for ResNet50\n",
        "            'block_inplanes': [64, 128, 256, 512] # Required: sequence of bottleneck internal channels; tunable with widen_factor (default=1.0)\n",
        "            # Output channels = block_inplanes × widen_factor × expansion\n",
        "            #                 = [64, 128, 256, 512] × 1.0 × 4\n",
        "            #                 = [256, 512, 1024, 2048]\n",
        "        }\n",
        "        model = ResNet(**params)\n",
        "    elif model_name == \"DenseNet121\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Default: spatial dimensions\n",
        "            'in_channels': num_input_channels, # Required: input channels\n",
        "            'out_channels': 1 # Required: regression output\n",
        "        }\n",
        "        model = DenseNet121(**params)\n",
        "    elif model_name == \"SEResNet50\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Required: spatial dimensions\n",
        "            'in_channels': num_input_channels, # Required: input channels\n",
        "            'num_classes': 1 # Non-default: regression output\n",
        "        }\n",
        "        model = SEResNet50(**params)\n",
        "    elif model_name == \"EfficientNetB0\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Non-default: spatial dimensions\n",
        "            'in_channels': num_input_channels, # Non-default: input channels\n",
        "            'num_classes': 1, # Non-default: regression output\n",
        "            'model_name': \"efficientnet-b0\" # Required: EfficientNet variant\n",
        "        }\n",
        "        model = EfficientNetBN(**params)\n",
        "    elif model_name == \"ViT\":\n",
        "        params = {\n",
        "            'spatial_dims': 3, # Default: spatial dimensions\n",
        "            'in_channels': num_input_channels, # Required: input channels\n",
        "            'num_classes': 1, # Non-default: regression output\n",
        "            'img_size': img_size, # Required: input image size\n",
        "            'patch_size': 16, # Required: spatial patch size for tokenization\n",
        "            'hidden_size': 768, # Default: transformer embedding dimension\n",
        "            'mlp_dim': 3072, # Default: MLP dimension (typically 4 × hidden_size)\n",
        "            'num_layers': 12, # Default: number of transformer blocks\n",
        "            'num_heads': 12, # Default: number of attention heads\n",
        "            'classification': True # Non-default: add prediction head (if Flase, feature extraction only)\n",
        "        }\n",
        "        model = ViT(**params)\n",
        "    elif model_name == \"SFCN\":\n",
        "        params = {\n",
        "            'input_channels': num_input_channels, # Required: input channels\n",
        "            'output_dim': 1, # Required: regression output\n",
        "            'channel_number': [32, 64, 128, 256, 256, 64], # Default: sequence of feature channels\n",
        "            'feature_dim': 64, # Default: refined feature dimension\n",
        "            'dropout': True # Default: use dropout for regularization\n",
        "        }\n",
        "        model = SFCN(**params)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
        "    return model\n",
        "\n",
        "def get_grad_scaler(device):\n",
        "    if device.type != \"cuda\":\n",
        "        return None\n",
        "    try: # Try newest API first (PyTorch 2.0+)\n",
        "        return torch.GradScaler(\"cuda\")\n",
        "    except (AttributeError, TypeError):\n",
        "        try: # Try torch.amp (PyTorch 1.10+)\n",
        "            return torch.amp.GradScaler(\"cuda\")\n",
        "        except (AttributeError, TypeError): # Fall back to old API\n",
        "            return torch.cuda.amp.GradScaler()\n",
        "\n",
        "def get_autocast_context(device, enabled=True):\n",
        "    if not enabled:\n",
        "        from contextlib import nullcontext\n",
        "        return nullcontext()\n",
        "    try:\n",
        "        # Try newest API first (PyTorch 2.0+)\n",
        "        return torch.autocast(device_type=device.type, dtype=torch.float16)\n",
        "    except (AttributeError, TypeError):\n",
        "        try:\n",
        "            # Try torch.amp (PyTorch 1.10+)\n",
        "            return torch.amp.autocast(device.type)\n",
        "        except (AttributeError, TypeError):\n",
        "            # Fall back to CUDA-specific (old)\n",
        "            if device.type == \"cuda\":\n",
        "                return torch.cuda.amp.autocast()\n",
        "            else:\n",
        "                from contextlib import nullcontext\n",
        "                return nullcontext()\n",
        "\n",
        "def train_one_epoch(model, device, train_loader, modalities, optimizer, criterion, scaler, metric):\n",
        "    model.train() # Set model to training mode\n",
        "    epoch_loss = 0.0\n",
        "    metric.reset()\n",
        "    for batch_data in train_loader:\n",
        "        # Prepare data\n",
        "        targets = batch_data['Age'].unsqueeze(1).to(device)\n",
        "        images = [batch_data[modality].to(device) for modality in modalities]\n",
        "        inputs = torch.cat(images, dim=1)\n",
        "        # Forward pass with mixed precision (if available)\n",
        "        optimizer.zero_grad()\n",
        "        with get_autocast_context(device, enabled=(scaler is not None)):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "        # Backward pass with gradient scaling (if available)\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # Accumulate metrics\n",
        "        epoch_loss += loss.item()\n",
        "        metric(y_pred=outputs, y=targets)\n",
        "    epoch_metric = metric.aggregate().item()\n",
        "    return epoch_loss / len(train_loader), epoch_metric\n",
        "\n",
        "def validate_one_epoch(model, device, val_loader, modalities, metric):\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for batch_data in val_loader:\n",
        "            targets = batch_data['Age'].unsqueeze(1).to(device)\n",
        "            images = [batch_data[modality].to(device) for modality in modalities]\n",
        "            inputs = torch.cat(images, dim=1)\n",
        "            outputs = model(inputs)\n",
        "            metric(y_pred=outputs, y=targets)\n",
        "    return metric.aggregate().item()\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=30, delta=0):\n",
        "        self.patience = patience # Number of epochs to wait before stopping\n",
        "        self.delta = delta # Minimum improvement threshold\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "    def __call__(self, metric):\n",
        "        score = metric\n",
        "        if self.best_score is None: # First epoch\n",
        "            self.best_score = score\n",
        "        elif score > self.best_score + self.delta: # Metric decreased\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else: # Metric improved\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "\n",
        "def train_model(model_dir, model, device, train_loader, val_loader, modalities, logger,\n",
        "        criterion, metric, max_epochs=100, learning_rate=1e-4, weight_decay=1e-5, val_interval=1, es_patience=30):\n",
        "    # Setup optimizer and learning rate scheduler\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
        "    scaler = get_grad_scaler(device)\n",
        "    start_time = time.time()\n",
        "    best_metric = float(\"inf\")\n",
        "    best_metric_epoch = -1\n",
        "    best_model_state = None\n",
        "    early_stopping = EarlyStopping(patience=es_patience, delta=0)\n",
        "    epoch_loss_values, epoch_metric_values, metric_values = [], [], []\n",
        "    for epoch in range(max_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        # Training phase\n",
        "        epoch_loss, epoch_metric = train_one_epoch(model, device, train_loader, modalities, optimizer, criterion, scaler, metric)\n",
        "        epoch_loss_values.append(epoch_loss)\n",
        "        epoch_metric_values.append(epoch_metric)\n",
        "        # Validation phase\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            val_metric = validate_one_epoch(model, device, val_loader, modalities, metric)\n",
        "            metric_values.append(val_metric)\n",
        "            # Save best model\n",
        "            if val_metric < best_metric:\n",
        "                best_metric = val_metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                best_model_state = model.state_dict()\n",
        "                torch.save(model.state_dict(), os.path.join(model_dir, \"BestMetricModel.pth\"))\n",
        "                logger.info(f\"Best MAE: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
        "            # Check early stopping\n",
        "            early_stopping(val_metric)\n",
        "            if early_stopping.early_stop:\n",
        "                logger.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "                print(f\"; Early stopping triggered at epoch {epoch + 1}\", end=\"\")\n",
        "                break\n",
        "        epoch_end_time = time.time()\n",
        "        logger.info(\n",
        "            f\"Epoch {epoch + 1} completed for {(epoch_end_time - epoch_start_time)/60:.2f} mins - \"\n",
        "            f\"Training loss: {epoch_loss:.4f}, Training MAE: {epoch_metric:.4f}, Validation MAE: {val_metric:.4f}\"\n",
        "        )\n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "        sys.stdout.write(f\"\\rEpoch {epoch + 1}/{max_epochs} completed\")\n",
        "        sys.stdout.flush()\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    logger.info(\n",
        "        f\"Best MAE: {best_metric:.3f} at epoch {best_metric_epoch}; \"\n",
        "        f\"Total time consumed: {total_time/60:.2f} mins\"\n",
        "    )\n",
        "    print(\n",
        "        f\"\\nBest MAE: {best_metric:.3f} at epoch {best_metric_epoch}; \"\n",
        "        f\"Total time consumed: {total_time/60:.2f} mins\"\n",
        "    )\n",
        "    # Load best model weights\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    return model, epoch_loss_values, epoch_metric_values, metric_values\n",
        "\n",
        "def plot_metric_values(model_dir, epoch_loss_values, epoch_metric_values, metric_values, val_interval=1):\n",
        "    _, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    axs[0].plot([i + 1 for i in range(len(epoch_loss_values))], epoch_loss_values, label='Training Loss', color='red')\n",
        "    axs[0].set_title('Training Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[1].plot([i + 1 for i in range(len(epoch_metric_values))], epoch_metric_values, label='Training MAE', color='red')\n",
        "    axs[1].plot([val_interval * (i + 1) for i in range(len(metric_values))], metric_values, label='Validation MAE', color='blue')\n",
        "    axs[1].set_title('Training MAE vs. Validation MAE')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_ylabel('MAE')\n",
        "    axs[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_dir, \"Performance.png\"), dpi=300)\n",
        "\n",
        "def select_representative_samples(dataset, age_thresholds=(40, 60)):\n",
        "    # Extract all ages\n",
        "    ages = []\n",
        "    indices = []\n",
        "    for idx in range(len(dataset)):\n",
        "        sample = dataset[idx]\n",
        "        ages.append(sample['Age'])\n",
        "        indices.append(idx)\n",
        "    ages = np.array(ages)\n",
        "    indices = np.array(indices)\n",
        "    young_thresh, old_thresh = age_thresholds\n",
        "    # Define groups\n",
        "    groups = {\n",
        "        'Young': (ages < young_thresh),\n",
        "        'Middle': (ages >= young_thresh) & (ages < old_thresh),\n",
        "        'Old': (ages >= old_thresh)\n",
        "    }\n",
        "    representative_samples = {}\n",
        "    for group_name, mask in groups.items():\n",
        "        if not mask.any():\n",
        "            print(f\"Warning: No samples in {group_name} group\")\n",
        "            continue\n",
        "        group_ages = ages[mask]\n",
        "        group_indices = indices[mask]\n",
        "        # Find sample closest to median\n",
        "        group_median = np.median(group_ages)\n",
        "        closest_idx = np.argmin(np.abs(group_ages - group_median))\n",
        "        representative_samples[group_name] = {\n",
        "            'index': group_indices[closest_idx],\n",
        "            'age': group_ages[closest_idx],\n",
        "            'group_median': group_median,\n",
        "            'group_range': (group_ages.min(), group_ages.max()),\n",
        "            'group_count': len(group_ages)\n",
        "        }\n",
        "    return representative_samples, ages\n",
        "\n",
        "class SHAP3D:\n",
        "    def __init__(self, model, modalities, device, batch_size=1):\n",
        "        self.model = model\n",
        "        self.modalities = modalities\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.explainer = None\n",
        "    def create_background(self, dl, num_samples=20): # Create background dataset for SHAP\n",
        "        dataset = dl.dataset\n",
        "        if num_samples is not None:\n",
        "            subset_indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "            subset_ds = Subset(dataset, subset_indices)\n",
        "            subset_dl = DataLoader(subset_ds, batch_size=self.batch_size, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        else:\n",
        "            subset_dl = dl\n",
        "        background_images = []\n",
        "        for batch_data in subset_dl:\n",
        "            images = [batch_data[modality].to(self.device) for modality in self.modalities]\n",
        "            inputs = torch.cat(images, dim=1)\n",
        "            background_images.append(inputs)\n",
        "        background_images = torch.cat(background_images, dim=0)\n",
        "        self.explainer = shap.GradientExplainer(self.model, background_images)\n",
        "    def compute_shap_values(self, dl, num_samples=None): # Compute SHAP values for samples\n",
        "        dataset = dl.dataset\n",
        "        if num_samples is not None:\n",
        "            subset_indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "            subset_ds = Subset(dataset, subset_indices)\n",
        "            subset_dl = DataLoader(subset_ds, batch_size=self.batch_size, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        else:\n",
        "            subset_dl = dl\n",
        "        shap_values_list = []\n",
        "        images_list = []\n",
        "        for batch_data in subset_dl:\n",
        "            images = [batch_data[modality].to(self.device) for modality in self.modalities]\n",
        "            inputs = torch.cat(images, dim=1)\n",
        "            shap_values = self.explainer.shap_values(inputs)\n",
        "            shap_values_list.append(shap_values)\n",
        "            images_list.append(inputs.cpu().numpy())\n",
        "        self.shap_values = np.concatenate(shap_values_list, axis=0)\n",
        "        self.images = np.concatenate(images_list, axis=0)\n",
        "    def visualize_shap(self, sample_img_path, shap_dir, vmin=-0.0025, vmax=0.0025): # Visualize mean absolute SHAP values using glass brain plots\n",
        "        reference_img = nib.load(sample_img_path)\n",
        "        os.makedirs(shap_dir, exist_ok=True)\n",
        "        _, axs = plt.subplots(len(self.modalities), 1, figsize=(12, len(self.modalities) * 4))\n",
        "        if len(self.modalities) == 1:\n",
        "            axs = [axs]\n",
        "        for i, modality in enumerate(self.modalities):\n",
        "            # Extract SHAP values for this modality\n",
        "            shap_values = self.shap_values[:, i, :, :, :, 0]\n",
        "            feature_values = self.images[:, i, :, :, :]\n",
        "            # Compute mean absolute SHAP values\n",
        "            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "            # Mask non-brain regions\n",
        "            common_mask = np.all(feature_values != 0, axis=0)\n",
        "            masked_shap = np.zeros_like(mean_abs_shap)\n",
        "            masked_shap[common_mask] = mean_abs_shap[common_mask]\n",
        "            # Save as NIfTI\n",
        "            shap_img = nib.Nifti1Image(\n",
        "                masked_shap, \n",
        "                affine=reference_img.affine,\n",
        "                header=reference_img.header\n",
        "            )\n",
        "            shap_img.header['descrip'] = 'Mean absolute SHAP values'\n",
        "            nib.save(shap_img, os.path.join(shap_dir, f\"MeanAbsSHAPValues_{modality}.nii.gz\"))\n",
        "            # Plot glass brain\n",
        "            plotting.plot_glass_brain(\n",
        "                shap_img, threshold=None, annotate=False,\n",
        "                plot_abs=False, black_bg='auto', axes=axs[i],\n",
        "                colorbar=True, cmap='black_red', symmetric_cbar=False,\n",
        "                alpha=0.3, vmin=vmin, vmax=vmax\n",
        "            )\n",
        "            axs[i].set_title(f\"{modality}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(shap_dir, \"SHAP_GlassBrain.png\"), dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "def apply_best_model(model_dir, model, device, test_loader, modalities, pred_dir, subjects):\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(torch.load(os.path.join(model_dir, \"BestMetricModel.pth\")))\n",
        "    model.eval()\n",
        "    os.makedirs(pred_dir, exist_ok=True)\n",
        "    pred_values = []\n",
        "    with torch.no_grad():\n",
        "        for batch_data in test_loader:\n",
        "            images = [batch_data[modality].to(device) for modality in modalities]\n",
        "            inputs = torch.cat(images, dim=1)\n",
        "            # Model inference\n",
        "            outputs = model(inputs)\n",
        "            pred_values.extend(outputs.cpu().numpy().flatten())\n",
        "    # Save predictions\n",
        "    pred_df = pd.DataFrame({\n",
        "        'ID': subjects,\n",
        "        'PredictedAge': pred_values\n",
        "    })\n",
        "    pred_df.to_csv(os.path.join(pred_dir, \"PredictedAge.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad1d539",
      "metadata": {},
      "source": [
        "### Prepare Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1613e1bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = os.path.join(\"AgePrediction\", \"Datasets\")\n",
        "model_dir_prefix = \"AgePrediction\"\n",
        "model_name = \"Regressor\" # any supported model name: Regressor, ResNet50, DenseNet121, SEResNet50, EfficientNetB0, ViT, SFCN\n",
        "modalities = [\"GM\", \"WM\", \"CSF\"]\n",
        "resize_dim = None # Use padding or specify tuple for resizing\n",
        "test_size = 0.2\n",
        "batch_size = 5\n",
        "max_epochs = 100\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-5\n",
        "val_interval = 1\n",
        "es_patience = 30\n",
        "\n",
        "# Setup output directory and logging\n",
        "model_dir = f\"{model_dir_prefix}_{model_name}_{'+'.join(modalities)}\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "log_file = os.path.join(model_dir, \"Training.log\")\n",
        "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(message)s\")\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bdd67ba",
      "metadata": {},
      "source": [
        "### Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b194a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "set_determinism(seed=0)\n",
        "train_loader, val_loader = load_data(\n",
        "    os.path.join(data_dir, \"train\"), modalities, batch_size, resize_dim=resize_dim, test_size=test_size,\n",
        "    inference=False, model_name=model_name\n",
        ")\n",
        "\n",
        "# Check data shape\n",
        "tr = first(train_loader)\n",
        "img_size = tuple(tr[modalities[0]].shape[-3:])\n",
        "print('\\nData shape for training:')\n",
        "for key, value in tr.items():\n",
        "    print(f'\\u2022 {key}: {tuple(value.shape)} × {len(train_loader)}')\n",
        "vl = first(val_loader)\n",
        "print('\\nData shape for validation:')\n",
        "for key, value in vl.items():\n",
        "    print(f'\\u2022 {key}: {tuple(value.shape)} × {len(val_loader)}')\n",
        "\n",
        "# Visualize data with adaptive sizing\n",
        "slice_index = img_size[2] // 2  # Middle slice\n",
        "num_modalities = len(modalities)\n",
        "if num_modalities <= 2:\n",
        "    fig_width = num_modalities * 6\n",
        "    fig_height = 6\n",
        "elif num_modalities <= 4:\n",
        "    fig_width = num_modalities * 4\n",
        "    fig_height = 5\n",
        "else:\n",
        "    fig_width = num_modalities * 3\n",
        "    fig_height = 4\n",
        "_, axs = plt.subplots(1, num_modalities, figsize=(fig_width, fig_height))\n",
        "if num_modalities == 1:\n",
        "    axs = [axs]\n",
        "for i, modality in enumerate(modalities):\n",
        "    image = tr[modality][0, 0, :, :, :].detach().cpu()\n",
        "    img_slice = torch.rot90(image[:, :, slice_index], k=1, dims=(0, 1))\n",
        "    axs[i].imshow(img_slice, cmap='gray')\n",
        "    axs[i].set_title(modality)\n",
        "    axs[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d37597",
      "metadata": {},
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad624f3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_model(model_name, len(modalities), img_size)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "model.to(device)\n",
        "print(f\"Selected model: {model_name}\")\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_params:,}\")\n",
        "criterion = nn.L1Loss()\n",
        "metric = MAEMetric(reduction=\"mean\")\n",
        "model, epoch_loss_values, epoch_metric_values, metric_values = train_model(\n",
        "    model_dir, model, device, train_loader, val_loader, modalities, logger,\n",
        "    criterion, metric, max_epochs, learning_rate, weight_decay, val_interval, es_patience\n",
        ")\n",
        "plot_metric_values(model_dir, epoch_loss_values, epoch_metric_values, metric_values, val_interval)\n",
        "\n",
        "# Visualize outcome\n",
        "representative_samples, all_ages = select_representative_samples(val_loader.dataset)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(all_ages, bins=20, alpha=0.7, edgecolor='black')\n",
        "for group_name, info in representative_samples.items():\n",
        "    color = {'Young': 'green', 'Middle': 'orange', 'Old': 'red'}[group_name]\n",
        "    plt.axvline(info['age'], color=color, linestyle='--', linewidth=2, label=f\"{group_name}: {info['age']:.1f} yrs\")\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Age Distribution with Representative Samples')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "model.eval()\n",
        "sample_info = []\n",
        "for group_name, info in representative_samples.items():\n",
        "    sample_info.append((info['index'], info['age'], group_name))\n",
        "sample_info.sort(key=lambda x: x[1])\n",
        "num_samples = len(sample_info)\n",
        "num_modalities = len(modalities)\n",
        "fig, axs = plt.subplots(num_samples, num_modalities, figsize=(num_modalities * 4, num_samples * 4))\n",
        "if num_samples == 1:\n",
        "    axs = axs.reshape(1, -1)\n",
        "if num_modalities == 1:\n",
        "    axs = axs.reshape(-1, 1)\n",
        "slice_index = img_size[2] // 2  # Middle slice\n",
        "for row, (sample_idx, true_age, group_name) in enumerate(sample_info):\n",
        "    with torch.no_grad():\n",
        "        sample = val_loader.dataset[sample_idx]\n",
        "        target = torch.tensor(sample['Age']).unsqueeze(0).unsqueeze(1).to(device)\n",
        "        images = [sample[modality].unsqueeze(0).to(device) for modality in modalities]\n",
        "        inputs = torch.cat(images, dim=1)\n",
        "        output = model(inputs)\n",
        "        pred_age = output.item()\n",
        "        error = pred_age - true_age\n",
        "    for col, modality in enumerate(modalities):\n",
        "        image = images[col][0, 0, :, :, :].detach().cpu()\n",
        "        img_slice = torch.rot90(image[:, :, slice_index], k=1, dims=(0, 1))\n",
        "        axs[row, col].imshow(img_slice, cmap='gray')\n",
        "        if col == 0:\n",
        "            axs[row, col].set_ylabel(f'{group_name}\\n({true_age:.0f} yrs)', fontsize=12)\n",
        "        axs[row, col].set_title(modality)\n",
        "        axs[row, col].axis('off')\n",
        "title_lines = []\n",
        "for sample_idx, true_age, group_name in sample_info:\n",
        "    with torch.no_grad():\n",
        "        sample = val_loader.dataset[sample_idx]\n",
        "        images = [sample[modality].unsqueeze(0).to(device) for modality in modalities]\n",
        "        inputs = torch.cat(images, dim=1)\n",
        "        output = model(inputs)\n",
        "        pred_age = output.item()\n",
        "        error = pred_age - true_age\n",
        "    title_lines.append(\n",
        "        f'{group_name}: True={true_age:.1f} yrs, Pred={pred_age:.1f} yrs, Error={error:+.1f} yrs'\n",
        "    )\n",
        "plt.suptitle('\\n'.join(title_lines), fontsize=11, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c6ae2ee",
      "metadata": {},
      "source": [
        "### SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b29ec4",
      "metadata": {},
      "outputs": [],
      "source": [
        "shap_analyzer = SHAP3D(model, modalities, device, batch_size=1)\n",
        "shap_analyzer.create_background(train_loader, num_samples=10)\n",
        "shap_analyzer.compute_shap_values(val_loader, num_samples=10)\n",
        "sample_img_path = os.path.join(data_dir, 'train', modalities[0], '001.nii.gz')\n",
        "shap_dir = os.path.join(model_dir, \"SHAP\")\n",
        "vmin = 0\n",
        "vmax = 0.01\n",
        "shap_analyzer.visualize_shap(sample_img_path, shap_dir, vmin=vmin, vmax=vmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "135f1006",
      "metadata": {},
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4c2a9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader, subjects = load_data(\n",
        "    os.path.join(data_dir, \"test\"), modalities, None, resize_dim=resize_dim, test_size=None,\n",
        "    inference=True, model_name=model_name\n",
        ")\n",
        "pred_dir = os.path.join(model_dir, \"Prediction\")\n",
        "apply_best_model(model_dir, model, device, test_loader, modalities, pred_dir, subjects)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Anonymous",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
