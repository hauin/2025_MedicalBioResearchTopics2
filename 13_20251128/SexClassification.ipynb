{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # %env CUDA_VISIBLE_DEVICES=0\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Subset\n",
        "import time\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, TransformerConv, global_mean_pool\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torchmetrics import Accuracy\n",
        "from monai.utils import set_determinism\n",
        "import nibabel as nib\n",
        "from nilearn import plotting\n",
        "import json\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BrainConnectivityDataset(Dataset):\n",
        "    def __init__(self, data_dir, model_name, node_feature_modalities=None, edge_feature_modalities=None, scaling='global', global_stats=None):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.node_feature_modalities = node_feature_modalities if node_feature_modalities else []\n",
        "        self.edge_feature_modalities = edge_feature_modalities if edge_feature_modalities else []\n",
        "        self.model_type = model_name\n",
        "        self.scaling = scaling\n",
        "        # Automatically determine edge construction based on model name\n",
        "        if model_name in ['GCN', 'GraphSAGE']:\n",
        "            self.edge_construction = 'sparse'\n",
        "        elif model_name in ['GAT', 'TransformerConv']:\n",
        "            self.edge_construction = 'dense'\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model name: {model_name}\")\n",
        "        # Load subject list from Subjects.csv\n",
        "        self.df = pd.read_csv(os.path.join(data_dir, 'Subjects.csv'))\n",
        "        self.subjects = self.df['ID'].to_numpy()\n",
        "        # Compute global statistics if global scaling is requested\n",
        "        if scaling == 'global':\n",
        "            if global_stats is not None:\n",
        "                self.global_stats = global_stats\n",
        "                print(\"Using provided global statistics for scaling.\")\n",
        "            else:\n",
        "                print(\"\\n\" + \"=\"*70)\n",
        "                print(\"Computing global statistics from training set...\")\n",
        "                print(\"=\"*70)\n",
        "                self.global_stats = self._compute_global_stats()\n",
        "                print(\"=\"*70 + \"\\n\")    \n",
        "        # Warning for multimodal edges with GCN/GraphSAGE\n",
        "        num_edge_modalities = len(self.edge_feature_modalities)\n",
        "        if model_name in ['GCN', 'GraphSAGE'] and num_edge_modalities > 1:\n",
        "            if model_name == 'GCN':\n",
        "                warnings.warn(\n",
        "                    f\"GCN only supports scalar edge weights. \"\n",
        "                    f\"Only the first ('{edge_feature_modalities[0]}') will be used as edge weights.\",\n",
        "                    UserWarning\n",
        "                )\n",
        "            elif model_name == 'GraphSAGE':\n",
        "                warnings.warn(\n",
        "                    f\"GraphSAGE does not use edge weights or features. \"\n",
        "                    f\"Only binary connectivity (edge_index) will be used.\",\n",
        "                    UserWarning\n",
        "                )\n",
        "    def _compute_global_stats(self): # Compute global mean and std from all training subjects\n",
        "        stats = {}\n",
        "        all_modalities = list(set(self.node_feature_modalities + self.edge_feature_modalities))\n",
        "        for modality in all_modalities:\n",
        "            print(f\"\\n{modality}:\")\n",
        "            all_values = []\n",
        "            for subject in self.subjects:\n",
        "                data = pd.read_csv(os.path.join(self.data_dir, modality, f\"{subject:03d}.csv\"), header=None).values\n",
        "                if data.ndim == 1 or (data.ndim == 2 and data.shape[1] == 1): # Vector: all values                  \n",
        "                    all_values.extend(data.flatten())\n",
        "                elif data.ndim == 2 and data.shape[0] == data.shape[1]: # Matrix: exclude diagonal and zeros                   \n",
        "                    mask = (np.abs(data) > 0) & (~np.eye(data.shape[0], dtype=bool))\n",
        "                    if mask.any():\n",
        "                        all_values.extend(data[mask])\n",
        "            all_values = np.array(all_values)\n",
        "            stats[modality] = {\n",
        "                'n_values': len(all_values),\n",
        "                'n_subjects': len(self.subjects),\n",
        "                'mean': np.mean(all_values),\n",
        "                'std': np.std(all_values),\n",
        "                'min': float(np.min(all_values)),\n",
        "                'max': float(np.max(all_values))\n",
        "            }\n",
        "            print(f\"  Data points: {stats[modality]['n_values']:,} from {stats[modality]['n_subjects']} subjects\")\n",
        "            print(f\"  Mean: {stats[modality]['mean']:.6f}\")\n",
        "            print(f\"  Std: {stats[modality]['std']:.6f}\")\n",
        "            print(f\"  Range: [{stats[modality]['min']:.6f}, {stats[modality]['max']:.6f}]\")  \n",
        "        return stats\n",
        "    def len(self):\n",
        "        return len(self.subjects)    \n",
        "    def get(self, idx):\n",
        "        subject = self.subjects[idx]        \n",
        "        # Determine number of nodes from first available modality\n",
        "        num_nodes = self._get_num_nodes(subject)        \n",
        "        # Construct node features\n",
        "        if self.node_feature_modalities: # Use provided modalities\n",
        "            x = self._load_node_features(subject, num_nodes)\n",
        "        else: # Dummy node features (all ones)\n",
        "            x = torch.ones(num_nodes, 1, dtype=torch.float)\n",
        "        # Construct edge features\n",
        "        if self.edge_feature_modalities: # Use provided modalities            \n",
        "            edge_matrices = self._load_edge_matrices(subject, num_nodes)            \n",
        "            # Convert to edge_index and edge_attr based on edge construction\n",
        "            if self.edge_construction == 'dense':\n",
        "                edge_index, edge_attr = self._matrices_to_edge_index_attr_dense(edge_matrices, num_nodes)\n",
        "            elif self.edge_construction == 'sparse':\n",
        "                edge_index, edge_attr = self._matrices_to_edge_index_attr_sparse(edge_matrices, num_nodes)\n",
        "        else: # Dummy edge features: fully connected with all ones\n",
        "            edge_index = self._create_fully_connected_edge_index(num_nodes)\n",
        "            edge_attr = torch.ones(edge_index.size(1), 1, dtype=torch.float)        \n",
        "        # Load label\n",
        "        label = torch.tensor(self.df['Sex'][idx], dtype=torch.long) if 'Sex' in self.df.columns else None\n",
        "        return Data(\n",
        "            x=x,\n",
        "            edge_index=edge_index,\n",
        "            edge_attr=edge_attr,\n",
        "            y=label,\n",
        "            num_nodes=num_nodes\n",
        "        )\n",
        "    def _get_num_nodes(self, subject): # Determine number of nodes from first available modality\n",
        "        all_modalities = self.node_feature_modalities + self.edge_feature_modalities\n",
        "        if not all_modalities:\n",
        "            raise ValueError(\"Must specify at least one modality in node_feature_modalities or edge_feature_modalities to determine graph size\")\n",
        "        first_modality = all_modalities[0]\n",
        "        data = pd.read_csv(\n",
        "            os.path.join(self.data_dir, first_modality, f\"{subject:03d}.csv\"),\n",
        "            header=None\n",
        "        ).values\n",
        "        if data.ndim == 2 and data.shape[0] == data.shape[1]:\n",
        "            return data.shape[0]\n",
        "        else:\n",
        "            return len(data.flatten())\n",
        "    def _load_node_features(self, subject, num_nodes): # Load and process node features from specified modalities\n",
        "        node_features_list = []        \n",
        "        for modality in self.node_feature_modalities:\n",
        "            data = pd.read_csv(os.path.join(self.data_dir, modality, f\"{subject:03d}.csv\"), header=None).values            \n",
        "            # Process based on data shape\n",
        "            if data.ndim == 1 or (data.ndim == 2 and data.shape[1] == 1): # Vector: use directly as node feature\n",
        "                node_feature = data.flatten()                \n",
        "                if len(node_feature) != num_nodes:\n",
        "                    raise ValueError(f\"Node feature dimension mismatch for {modality}: expected {num_nodes}, got {len(node_feature)}\")                \n",
        "                # Apply scaling\n",
        "                if self.scaling == 'global':\n",
        "                    node_feature = self._scale_global(node_feature, modality)\n",
        "                elif self.scaling == 'per_subject':\n",
        "                    node_feature = self._scale_persubject(node_feature)\n",
        "            elif data.ndim == 2 and data.shape[0] == data.shape[1]: # Matrix: scale first, then compute node feature\n",
        "                if data.shape[0] != num_nodes:\n",
        "                    raise ValueError(f\"Matrix size mismatch for {modality}: expected ({num_nodes}, {num_nodes}), got {data.shape}\")\n",
        "                # Apply scaling\n",
        "                if self.scaling == 'global':\n",
        "                    data = self._scale_global(data, modality, is_matrix=True)\n",
        "                elif self.scaling == 'per_subject':\n",
        "                    data = self._scale_persubject(data, is_matrix=True)\n",
        "                # Compute node feature from edges\n",
        "                node_feature = self._compute_node_feature_from_edges(data)\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid data shape for {modality}: {data.shape}: expected (N,) or (N, N)\")\n",
        "            node_features_list.append(torch.tensor(node_feature, dtype=torch.float).unsqueeze(1))\n",
        "        return torch.cat(node_features_list, dim=1)  # [num_nodes, num_node_features]\n",
        "    def _load_edge_matrices(self, subject, num_nodes): # Load and scale edge matrices from specified modalities\n",
        "        edge_matrices = {}\n",
        "        for modality in self.edge_feature_modalities:\n",
        "            conn_matrix = pd.read_csv(os.path.join(self.data_dir, modality, f\"{subject:03d}.csv\"), header=None).values\n",
        "            if not (conn_matrix.ndim == 2 and conn_matrix.shape[0] == conn_matrix.shape[1]):\n",
        "                raise ValueError(f\"Edge feature dimension mismatch for {modality}: got {conn_matrix.shape}\")\n",
        "            if conn_matrix.shape[0] != num_nodes:\n",
        "                raise ValueError(f\"Matrix size mismatch for {modality}: expected ({num_nodes}, {num_nodes}), got {conn_matrix.shape}\")\n",
        "            # Apply scaling\n",
        "            if self.scaling == 'global':\n",
        "                conn_matrix = self._scale_global(conn_matrix, modality, is_matrix=True)\n",
        "            elif self.scaling == 'per_subject':\n",
        "                conn_matrix = self._scale_persubject(conn_matrix, is_matrix=True)\n",
        "            edge_matrices[modality] = conn_matrix\n",
        "        return edge_matrices \n",
        "    def _scale_global(self, data, modality, is_matrix=False): # Scale data using global statistics: (data - global_mean) / global_std\n",
        "        mean = self.global_stats[modality]['mean']\n",
        "        std = self.global_stats[modality]['std']\n",
        "        if is_matrix: # Matrix: only scale non-zero values to preserve sparsity \n",
        "            mask = (np.abs(data) > 0) & (~np.eye(data.shape[0], dtype=bool))\n",
        "            scaled_data = np.zeros_like(data, dtype=np.float64)\n",
        "            if mask.any():\n",
        "                scaled_data[mask] = (data[mask] - mean) / (std + 1e-8)\n",
        "            np.fill_diagonal(scaled_data, 1) # Preserve diagonal\n",
        "        else: # Vector: scale all values\n",
        "            scaled_data = (data - mean) / (std + 1e-8)\n",
        "        return scaled_data\n",
        "    def _scale_persubject(self, data, is_matrix=False): # Scale data to [-1, 1] range per subject\n",
        "        if is_matrix: # Matrix: exclude diagonal\n",
        "            values = data[~np.eye(data.shape[0], dtype=bool)]\n",
        "            absmax = np.abs(values).max()\n",
        "            scaled_data = data / (absmax + 1e-8)\n",
        "            np.fill_diagonal(scaled_data, 1)\n",
        "        else: # Vector\n",
        "            absmax = np.abs(data).max()\n",
        "            scaled_data = data / (absmax + 1e-8)\n",
        "        return scaled_data\n",
        "    def _compute_node_feature_from_edges(self, conn_matrix): # Compute node feature as mean of all connected edges (excluding diagonal)\n",
        "        num_nodes = conn_matrix.shape[0]\n",
        "        node_feature = np.zeros(num_nodes)    \n",
        "        for i in range(num_nodes):\n",
        "            connections = np.concatenate([conn_matrix[i, :i], conn_matrix[i, i+1:]])\n",
        "            node_feature[i] = np.mean(connections)       \n",
        "        return node_feature   \n",
        "    def _matrices_to_edge_index_attr_dense(self, edge_matrices, num_nodes): # Create dense edge_index with all possible edges (no self-loops)\n",
        "        modalities = list(edge_matrices.keys())\n",
        "        num_modalities = len(modalities)        \n",
        "        # Create all possible edges (excluding self-loops)\n",
        "        edge_list = []\n",
        "        for i in range(num_nodes):\n",
        "            for j in range(num_nodes):\n",
        "                if i != j:\n",
        "                    edge_list.append([i, j])        \n",
        "        num_edges = len(edge_list)\n",
        "        edge_index = torch.tensor(edge_list, dtype=torch.long).t() # [2, num_edges] where num_edges = num_nodes * (num_nodes - 1)  \n",
        "        # Extract edge attributes for all edges\n",
        "        edge_attr = torch.zeros(num_edges, num_modalities, dtype=torch.float) # [num_edges, num_modalities]\n",
        "        for mod_idx, modality in enumerate(modalities):\n",
        "            matrix = edge_matrices[modality]\n",
        "            for edge_idx, (i, j) in enumerate(edge_list):\n",
        "                edge_attr[edge_idx, mod_idx] = matrix[i, j]\n",
        "        return edge_index, edge_attr    \n",
        "    def _matrices_to_edge_index_attr_sparse(self, edge_matrices, num_nodes): # Create sparse edge_index with non-zero edges only (no self-loops)\n",
        "        modalities = list(edge_matrices.keys())\n",
        "        num_modalities = len(modalities)        \n",
        "        # Use first modality to determine which edges exist\n",
        "        primary_modality = modalities[0]\n",
        "        primary_matrix = edge_matrices[primary_modality]        \n",
        "        # Create mask for non-zero, non-diagonal entries\n",
        "        mask = (np.abs(primary_matrix) > 0) & (~np.eye(num_nodes, dtype=bool))\n",
        "        edge_indices = np.argwhere(mask)\n",
        "        num_edges = len(edge_indices)        \n",
        "        edge_index = torch.tensor(edge_indices.T, dtype=torch.long) # [2, num_edges] where num_edges = number of non-zero edges    \n",
        "        # Extract edge attributes from all modalities\n",
        "        edge_attr = torch.zeros(num_edges, num_modalities, dtype=torch.float) # [num_edges, num_modalities]\n",
        "        for mod_idx, modality in enumerate(modalities):\n",
        "            matrix = edge_matrices[modality]\n",
        "            for edge_idx, (i, j) in enumerate(edge_indices):\n",
        "                edge_attr[edge_idx, mod_idx] = matrix[i, j]        \n",
        "        return edge_index, edge_attr  \n",
        "    def _create_fully_connected_edge_index(self, num_nodes): # Create fully connected edge_index (no self-loops)\n",
        "        edge_index = []\n",
        "        for i in range(num_nodes):\n",
        "            for j in range(num_nodes):\n",
        "                if i != j:\n",
        "                    edge_index.append([i, j])\n",
        "        return torch.tensor(edge_index, dtype=torch.long).t()\n",
        "    \n",
        "def load_data(dataset, batch_size, inference=False, test_size=0.2):\n",
        "    if inference: # Test\n",
        "        test_loader = DataLoader(\n",
        "            dataset, \n",
        "            batch_size=1, \n",
        "            num_workers=0, \n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "        subjects = dataset.subjects\n",
        "        return test_loader, subjects\n",
        "    else: # Training/Validation\n",
        "        # Split data into train and validation sets with stratification\n",
        "        indices = list(range(len(dataset)))\n",
        "        labels = [dataset[i].y.item() for i in indices]\n",
        "        train_indices, val_indices = train_test_split(indices, test_size=test_size, stratify=labels, random_state=42)\n",
        "        train_ds = Subset(dataset, train_indices)\n",
        "        val_ds = Subset(dataset, val_indices)\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "        return train_loader, val_loader\n",
        "\n",
        "class BrainGNN(torch.nn.Module):\n",
        "    def __init__(self, model_type, num_node_features, num_edge_features, \n",
        "                 num_channels=[32, 32, 64, 64, 128], **kwargs):\n",
        "        super().__init__()\n",
        "        self.model_type = model_type\n",
        "        heads = kwargs.get('heads', 4) # For attention-based models\n",
        "        # Adjust channels for multi-head attention\n",
        "        if model_type in ['GAT', 'TransformerConv']:\n",
        "            for channel in num_channels:\n",
        "                if channel % heads != 0:\n",
        "                    raise ValueError(f\"All channel numbers must be divisible by number of heads ({heads}): got channel={channel}\")\n",
        "            channels_to_use = [c // heads for c in num_channels]\n",
        "        else:\n",
        "            channels_to_use = num_channels\n",
        "        # Build convolution layers\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList()          \n",
        "        # First layer\n",
        "        if model_type == 'GCN':\n",
        "            self.conv_layers.append(GCNConv(num_node_features, channels_to_use[0]))\n",
        "        elif model_type == 'GraphSAGE':\n",
        "            self.conv_layers.append(SAGEConv(num_node_features, channels_to_use[0]))\n",
        "        elif model_type == 'GAT':\n",
        "            self.conv_layers.append(GATConv(num_node_features, channels_to_use[0], heads=heads, edge_dim=num_edge_features))\n",
        "        elif model_type == 'TransformerConv':\n",
        "            self.conv_layers.append(TransformerConv(num_node_features, channels_to_use[0], heads=heads, edge_dim=num_edge_features))\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "        # Batch norm for first layer\n",
        "        if model_type in ['GAT', 'TransformerConv']:\n",
        "            self.batch_norms.append(nn.BatchNorm1d(channels_to_use[0] * heads))\n",
        "        else:\n",
        "            self.batch_norms.append(nn.BatchNorm1d(channels_to_use[0]))\n",
        "        # Subsequent layers\n",
        "        for i in range(len(channels_to_use) - 1):\n",
        "            num_in_channels = channels_to_use[i] * (heads if model_type in ['GAT', 'TransformerConv'] else 1)\n",
        "            if model_type == 'GCN':\n",
        "                self.conv_layers.append(GCNConv(num_in_channels, channels_to_use[i + 1]))\n",
        "            elif model_type == 'GraphSAGE':\n",
        "                self.conv_layers.append(SAGEConv(num_in_channels, channels_to_use[i + 1]))\n",
        "            elif model_type == 'GAT':\n",
        "                self.conv_layers.append(GATConv(num_in_channels, channels_to_use[i + 1], heads=heads, edge_dim=num_edge_features))\n",
        "            elif model_type == 'TransformerConv':\n",
        "                self.conv_layers.append(TransformerConv(num_in_channels, channels_to_use[i + 1], heads=heads, edge_dim=num_edge_features))\n",
        "            # Batch norm\n",
        "            if model_type in ['GAT', 'TransformerConv']:\n",
        "                self.batch_norms.append(nn.BatchNorm1d(channels_to_use[i + 1] * heads))\n",
        "            else:\n",
        "                self.batch_norms.append(nn.BatchNorm1d(channels_to_use[i + 1]))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        # Fully connected layers\n",
        "        num_final_channels = channels_to_use[-1] * (heads if model_type in ['GAT', 'TransformerConv'] else 1)\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(num_final_channels, num_final_channels // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(num_final_channels // 2, num_final_channels // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(num_final_channels // 4, 2)  # Binary classification\n",
        "        )\n",
        "    def forward(self, data):\n",
        "        x = data.x\n",
        "        edge_index = data.edge_index\n",
        "        edge_attr = data.edge_attr  \n",
        "        # Graph convolution layers with residual connections\n",
        "        for i, (conv, bn) in enumerate(zip(self.conv_layers, self.batch_norms)):\n",
        "            identity = x   \n",
        "            # Apply convolution\n",
        "            if self.model_type == 'GCN':\n",
        "                # GCN: Use edge_attr as edge_weight (must be scalar)\n",
        "                edge_weight = edge_attr[:, 0] if edge_attr.dim() > 1 else edge_attr\n",
        "                x = conv(x, edge_index, edge_weight=edge_weight)\n",
        "            elif self.model_type == 'GraphSAGE':\n",
        "                # GraphSAGE: No edge features supported\n",
        "                x = conv(x, edge_index)\n",
        "            else: # GAT or TransformerConv\n",
        "                # GAT/TransformerConv: Use edge_attr as vector features\n",
        "                x = conv(x, edge_index, edge_attr=edge_attr) \n",
        "            x = bn(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.dropout(x)\n",
        "            # Residual connection (skip connection)\n",
        "            if i > 0 and x.size() == identity.size():\n",
        "                x = x + identity\n",
        "        # Global pooling\n",
        "        x = global_mean_pool(x, data.batch)\n",
        "        # Classification head\n",
        "        x = self.fc_layers(x)\n",
        "        return x\n",
        "\n",
        "def get_model(model_name, num_node_features, num_edge_features, num_channels=[32, 32, 64, 64, 128], heads=4):\n",
        "    # Validate model name\n",
        "    supported_models = ['GCN', 'GraphSAGE', 'GAT', 'TransformerConv']\n",
        "    if model_name not in supported_models:\n",
        "        raise ValueError(f\"Unsupported model name: {model_name}\")\n",
        "    # Common parameters for all models\n",
        "    params = {\n",
        "        'model_type': model_name,\n",
        "        'num_node_features': num_node_features,\n",
        "        'num_edge_features': num_edge_features,\n",
        "        'num_channels': num_channels\n",
        "    }\n",
        "    # Add heads only for attention-based models\n",
        "    if model_name in ['GAT', 'TransformerConv']:\n",
        "        params['heads'] = heads\n",
        "    return BrainGNN(**params)\n",
        "\n",
        "def visualize_conn_matrix(sample, edge_feature_modalities, title=None, graph_idx=0):\n",
        "    n_modalities = len(edge_feature_modalities)\n",
        "    _, axs = plt.subplots(1, n_modalities, \n",
        "                          figsize=(n_modalities * 4, 4), \n",
        "                          squeeze=False)\n",
        "    # Step 1: Get graph boundaries from ptr\n",
        "    batch_size = len(sample.ptr) - 1\n",
        "    if graph_idx >= batch_size:\n",
        "        raise ValueError(f\"graph_idx {graph_idx} >= batch_size {batch_size}\")\n",
        "    node_start = sample.ptr[graph_idx].item()\n",
        "    node_end = sample.ptr[graph_idx + 1].item()\n",
        "    num_nodes = node_end - node_start\n",
        "    # Step 2: Find edges belonging to this graph\n",
        "    edge_mask = (sample.edge_index[0] >= node_start) & \\\n",
        "                (sample.edge_index[0] < node_end) & \\\n",
        "                (sample.edge_index[1] >= node_start) & \\\n",
        "                (sample.edge_index[1] < node_end)\n",
        "    #  Step 3: Extract edges and attributes\n",
        "    edge_index_graph = sample.edge_index[:, edge_mask]\n",
        "    edge_attr_graph = sample.edge_attr[edge_mask]\n",
        "    # Step 4: Convert to local indexing (0 to num_nodes-1)\n",
        "    edge_index_local = edge_index_graph - node_start\n",
        "    # Visualize each modality\n",
        "    for col, modality in enumerate(edge_feature_modalities):\n",
        "        edge_attr = edge_attr_graph[:, col]\n",
        "        # Initialize with zeros\n",
        "        conn_matrix = torch.zeros((num_nodes, num_nodes), \n",
        "                                  dtype=torch.float32,\n",
        "                                  device=edge_attr.device)\n",
        "        # Fill in edge values\n",
        "        conn_matrix[edge_index_local[0], edge_index_local[1]] = edge_attr\n",
        "        # Convert to numpy for plotting\n",
        "        matrix_numpy = conn_matrix.cpu().numpy()\n",
        "        # Color scale\n",
        "        non_zero_values = matrix_numpy[matrix_numpy != 0]\n",
        "        if len(non_zero_values) > 0:\n",
        "            abs_max = np.abs(non_zero_values).max()\n",
        "            has_negative = np.any(non_zero_values < 0)\n",
        "        else:\n",
        "            abs_max = 1\n",
        "            has_negative = False\n",
        "        # Plot\n",
        "        ax = axs[0, col]\n",
        "        im = ax.imshow(\n",
        "            matrix_numpy, \n",
        "            cmap='RdBu_r' if has_negative else 'hot',\n",
        "            vmin=-abs_max if has_negative else 0, \n",
        "            vmax=abs_max,\n",
        "            aspect='equal'\n",
        "        )\n",
        "        ax.set_title(f'{modality}', fontsize=11)\n",
        "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "        ax.tick_params(left=False, bottom=False, \n",
        "                      labelleft=False, labelbottom=False)\n",
        "    if title:\n",
        "        plt.suptitle(f\"{title} (Graph {graph_idx})\", fontsize=13, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def get_grad_scaler(device):\n",
        "    if device.type != \"cuda\":\n",
        "        return None\n",
        "    try: # Try newest API first (PyTorch 2.0+)\n",
        "        return torch.GradScaler(\"cuda\")\n",
        "    except (AttributeError, TypeError):\n",
        "        try: # Try torch.amp (PyTorch 1.10+)\n",
        "            return torch.amp.GradScaler(\"cuda\")\n",
        "        except (AttributeError, TypeError): # Fall back to old API\n",
        "            return torch.cuda.amp.GradScaler()\n",
        "\n",
        "def get_autocast_context(device, enabled=True):\n",
        "    if not enabled:\n",
        "        from contextlib import nullcontext\n",
        "        return nullcontext()\n",
        "    try:\n",
        "        # Try newest API first (PyTorch 2.0+)\n",
        "        return torch.autocast(device_type=device.type, dtype=torch.float16)\n",
        "    except (AttributeError, TypeError):\n",
        "        try:\n",
        "            # Try torch.amp (PyTorch 1.10+)\n",
        "            return torch.amp.autocast(device.type)\n",
        "        except (AttributeError, TypeError):\n",
        "            # Fall back to CUDA-specific (old)\n",
        "            if device.type == \"cuda\":\n",
        "                return torch.cuda.amp.autocast()\n",
        "            else:\n",
        "                from contextlib import nullcontext\n",
        "                return nullcontext()\n",
        "\n",
        "def train_one_epoch(model, device, train_loader, optimizer, criterion, scaler, metric):\n",
        "    model.train() # Set model to training mode\n",
        "    epoch_loss = 0.0\n",
        "    metric.reset()\n",
        "    for batch_data in train_loader:\n",
        "        # Prepare data\n",
        "        batch_data = batch_data.to(device)\n",
        "        # Forward pass with mixed precision (if available)\n",
        "        optimizer.zero_grad()\n",
        "        with get_autocast_context(device, enabled=(scaler is not None)):\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_data.y)\n",
        "        # Backward pass with gradient scaling (if available)\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        # Accumulate metrics\n",
        "        epoch_loss += loss.item()\n",
        "        metric(outputs[:, 1], batch_data.y)\n",
        "    epoch_metric = metric.compute().item()\n",
        "    return epoch_loss / len(train_loader), epoch_metric\n",
        "\n",
        "def validate_one_epoch(model, device, val_loader, metric):\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "    with torch.no_grad():\n",
        "        for batch_data in val_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            outputs = model(batch_data)\n",
        "            metric(outputs[:, 1], batch_data.y)\n",
        "    return metric.compute().item()\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=30, delta=0):\n",
        "        self.patience = patience # Number of epochs to wait before stopping\n",
        "        self.delta = delta # Minimum improvement threshold\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "    def __call__(self, metric):\n",
        "        score = metric\n",
        "        if self.best_score is None: # First epoch\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score + self.delta: # Metric decreased\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else: # Metric improved\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "\n",
        "def train_model(model_dir, model, device, train_loader, val_loader, logger,\n",
        "        criterion, metric, max_epochs=100, learning_rate=1e-4, weight_decay=1e-5, val_interval=1, es_patience=30):\n",
        "    # Setup optimizer and learning rate scheduler\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
        "    scaler = get_grad_scaler(device)\n",
        "    start_time = time.time()\n",
        "    best_metric = -1\n",
        "    best_metric_epoch = -1\n",
        "    best_model_state = None\n",
        "    early_stopping = EarlyStopping(patience=es_patience, delta=0)\n",
        "    epoch_loss_values, epoch_metric_values, metric_values = [], [], []\n",
        "    for epoch in range(max_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        # Training phase\n",
        "        epoch_loss, epoch_metric = train_one_epoch(model, device, train_loader, optimizer, criterion, scaler, metric)\n",
        "        epoch_loss_values.append(epoch_loss)\n",
        "        epoch_metric_values.append(epoch_metric)\n",
        "        # Validation phase\n",
        "        if (epoch + 1) % val_interval == 0:\n",
        "            val_metric = validate_one_epoch(model, device, val_loader, metric)\n",
        "            metric_values.append(val_metric)\n",
        "            # Save best model\n",
        "            if val_metric > best_metric:\n",
        "                best_metric = val_metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                best_model_state = model.state_dict()\n",
        "                torch.save(model.state_dict(), os.path.join(model_dir, \"BestMetricModel.pth\"))\n",
        "                logger.info(f\"Best accuracy: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
        "            # Check early stopping\n",
        "            early_stopping(val_metric)\n",
        "            if early_stopping.early_stop:\n",
        "                logger.info(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "                print(f\"; Early stopping triggered at epoch {epoch + 1}\", end=\"\")\n",
        "                break\n",
        "        epoch_end_time = time.time()\n",
        "        logger.info(\n",
        "            f\"Epoch {epoch + 1} completed for {(epoch_end_time - epoch_start_time)/60:.2f} mins - \"\n",
        "            f\"Training loss: {epoch_loss:.4f}, Training accuracy: {epoch_metric:.4f}, Validation accuracy: {val_metric:.4f}\"\n",
        "        )\n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "        sys.stdout.write(f\"\\rEpoch {epoch + 1}/{max_epochs} completed\")\n",
        "        sys.stdout.flush()\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    logger.info(\n",
        "        f\"Best accuracy: {best_metric:.3f} at epoch {best_metric_epoch}; \"\n",
        "        f\"Total time consumed: {total_time/60:.2f} mins\"\n",
        "    )\n",
        "    print(\n",
        "        f\"\\nBest accuracy: {best_metric:.3f} at epoch {best_metric_epoch}; \"\n",
        "        f\"Total time consumed: {total_time/60:.2f} mins\"\n",
        "    )\n",
        "    # Load best model weights\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    return model, epoch_loss_values, epoch_metric_values, metric_values\n",
        "\n",
        "def plot_metric_values(model_dir, epoch_loss_values, epoch_metric_values, metric_values, val_interval=1):\n",
        "    _, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    axs[0].plot([i + 1 for i in range(len(epoch_loss_values))], epoch_loss_values, label='Training Loss', color='red')\n",
        "    axs[0].set_title('Training Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[1].plot([i + 1 for i in range(len(epoch_metric_values))], epoch_metric_values, label='Training Accuracy', color='red')\n",
        "    axs[1].plot([val_interval * (i + 1) for i in range(len(metric_values))], metric_values, label='Validation Accuracy', color='blue')\n",
        "    axs[1].set_title('Training Accuracy vs. Validation Accuracy')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "    axs[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(model_dir, \"Performance.png\"), dpi=300)\n",
        "\n",
        "def occlusion_based_sensitivity(model, sample, style='diverging', directed=False, window_size=10):\n",
        "    # Compute sensitivity by systematically occluding edges\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    sample = sample.to(device)\n",
        "    # Get baseline prediction\n",
        "    baseline_output = model(sample)\n",
        "    baseline_pred = F.softmax(baseline_output, dim=1)[0, 1].item()\n",
        "    num_nodes_per_graph = sample.num_nodes\n",
        "    sensitivity_matrix = torch.zeros((num_nodes_per_graph, num_nodes_per_graph), device=device)\n",
        "    # Define node pairs to evaluate\n",
        "    if directed:\n",
        "        node_pairs = [(i, j) for i in range(num_nodes_per_graph) for j in range(num_nodes_per_graph) if i != j]\n",
        "    else:\n",
        "        node_pairs = [(i, j) for i in range(num_nodes_per_graph) for j in range(i+1, num_nodes_per_graph)]\n",
        "    # Process in windows for efficiency\n",
        "    for window_start in range(0, len(node_pairs), window_size):\n",
        "        window_end = min(window_start + window_size, len(node_pairs))\n",
        "        current_pairs = node_pairs[window_start:window_end]\n",
        "        # Create temporary data with occluded edges\n",
        "        temp_data = Data(\n",
        "            x=sample.x,\n",
        "            edge_index=sample.edge_index,\n",
        "            edge_attr=sample.edge_attr.clone(),\n",
        "            y=sample.y,\n",
        "            num_nodes=sample.num_nodes\n",
        "        ).to(device)\n",
        "        # Mask edges\n",
        "        mask = torch.zeros_like(temp_data.edge_index[0], dtype=torch.bool)\n",
        "        for i, j in current_pairs:\n",
        "            if directed:\n",
        "                mask |= (temp_data.edge_index[0] == i) & (temp_data.edge_index[1] == j)\n",
        "            else:\n",
        "                mask |= ((temp_data.edge_index[0] == i) & (temp_data.edge_index[1] == j)) | \\\n",
        "                        ((temp_data.edge_index[0] == j) & (temp_data.edge_index[1] == i))\n",
        "        temp_data.edge_attr[mask] = 0\n",
        "        # Compute sensitivity\n",
        "        with torch.no_grad():\n",
        "            output = model(temp_data)\n",
        "            pred = F.softmax(output, dim=1)[0, 1].item()\n",
        "        if style == 'diverging':\n",
        "            sensitivity = pred - baseline_pred\n",
        "        elif style == 'absolute':\n",
        "            sensitivity = abs(pred - baseline_pred)\n",
        "        # Update sensitivity matrix\n",
        "        for i, j in current_pairs:\n",
        "            sensitivity_matrix[i, j] = sensitivity\n",
        "            if not directed:\n",
        "                sensitivity_matrix[j, i] = sensitivity\n",
        "    return sensitivity_matrix\n",
        "\n",
        "def occlusion_sensitivity_analysis(model, loader, model_dir, edge_feature_modalities, \n",
        "        style='diverging', directed=False, num_samples=20, labels_file='brainnetome_labels.csv'):\n",
        "    # Load brain region labels\n",
        "    try:\n",
        "        df_labels = pd.read_csv(labels_file)\n",
        "        labels_dict = dict(zip(df_labels['Index'] - 1, df_labels['Abbreviation']))  # 0-indexed\n",
        "        full_names_dict = dict(zip(df_labels['Index'] - 1, df_labels['Full_Name']))\n",
        "        print(f\"Loaded {len(df_labels)} brain region labels from {labels_file}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {labels_file} not found. Using node indices only.\")\n",
        "        labels_dict = None\n",
        "        full_names_dict = None\n",
        "    accumulated_sensitivity = None\n",
        "    accumulated_samples = 0\n",
        "    sampling_complete = False\n",
        "    for batch_data in loader:\n",
        "        if sampling_complete:\n",
        "            break\n",
        "        batch_size = len(batch_data.ptr) - 1\n",
        "        for graph_idx in range(batch_size):\n",
        "            if accumulated_samples >= num_samples:\n",
        "                sampling_complete = True\n",
        "                break\n",
        "            # Extract single graph from batch\n",
        "            start_idx = batch_data.ptr[graph_idx].item()\n",
        "            end_idx = batch_data.ptr[graph_idx + 1].item()\n",
        "            sample_x = batch_data.x[start_idx:end_idx, :]\n",
        "            edge_mask = (batch_data.edge_index[0] >= start_idx) & (batch_data.edge_index[0] < end_idx)\n",
        "            sample_edge_index = batch_data.edge_index[:, edge_mask] - start_idx\n",
        "            sample_edge_attr = batch_data.edge_attr[edge_mask, :]\n",
        "            sample_y = batch_data.y[graph_idx]\n",
        "            sample_num_nodes = end_idx - start_idx\n",
        "            sample = Data(\n",
        "                x=sample_x,\n",
        "                edge_index=sample_edge_index,\n",
        "                edge_attr=sample_edge_attr,\n",
        "                y=sample_y,\n",
        "                num_nodes=sample_num_nodes,\n",
        "                batch=torch.zeros(sample_num_nodes, dtype=torch.long)\n",
        "            )\n",
        "            accumulated_samples += 1\n",
        "            # Compute sensitivity for this sample\n",
        "            sensitivity_matrix = occlusion_based_sensitivity(\n",
        "                model, sample, style=style, directed=directed\n",
        "            )\n",
        "            if accumulated_sensitivity is None:\n",
        "                accumulated_sensitivity = sensitivity_matrix\n",
        "            else:\n",
        "                accumulated_sensitivity += sensitivity_matrix\n",
        "    # Average sensitivity across samples\n",
        "    mean_sensitivity = accumulated_sensitivity / num_samples\n",
        "    # Save sensitivity matrix\n",
        "    df_sensitivity = pd.DataFrame(mean_sensitivity.cpu().numpy())\n",
        "    df_sensitivity.to_csv(\n",
        "        os.path.join(model_dir, f\"SensitivityMap_{'_'.join(edge_feature_modalities)}.csv\"), \n",
        "        index=False, \n",
        "        header=False\n",
        "    )\n",
        "    # Extract top 5% most sensitive edges\n",
        "    num_nodes_per_graph = mean_sensitivity.shape[0]\n",
        "    if directed:\n",
        "        values = mean_sensitivity.view(-1)\n",
        "        mask = ~torch.eye(num_nodes_per_graph, dtype=torch.bool, device=mean_sensitivity.device).view(-1)\n",
        "        values = values[mask]\n",
        "        abs_values = torch.abs(values)\n",
        "        threshold = torch.quantile(abs_values, 0.95)\n",
        "        high_sensitivity_pairs = []\n",
        "        for i in range(num_nodes_per_graph):\n",
        "            for j in range(num_nodes_per_graph):\n",
        "                if i != j and abs(mean_sensitivity[i, j]) >= threshold:\n",
        "                    # Add region labels\n",
        "                    if labels_dict:\n",
        "                        node_i_label = labels_dict.get(i, f\"Node_{i+1}\")\n",
        "                        node_j_label = labels_dict.get(j, f\"Node_{j+1}\")\n",
        "                    else:\n",
        "                        node_i_label = f\"Node_{i+1}\"\n",
        "                        node_j_label = f\"Node_{j+1}\"\n",
        "                    high_sensitivity_pairs.append({\n",
        "                        'Node_i': i + 1,\n",
        "                        'Node_i_Label': node_i_label,\n",
        "                        'Node_j': j + 1,\n",
        "                        'Node_j_Label': node_j_label,\n",
        "                        'Sensitivity': mean_sensitivity[i, j].item()\n",
        "                    })\n",
        "    else:\n",
        "        upper_tri = torch.triu(mean_sensitivity, diagonal=1)\n",
        "        values = upper_tri.view(-1)\n",
        "        values = values[values != 0]\n",
        "        abs_values = torch.abs(values)\n",
        "        threshold = torch.quantile(abs_values, 0.95)\n",
        "        high_sensitivity_pairs = []\n",
        "        for i in range(num_nodes_per_graph):\n",
        "            for j in range(i+1, num_nodes_per_graph):\n",
        "                if abs(mean_sensitivity[i, j]) >= threshold:\n",
        "                    # Add region labels\n",
        "                    if labels_dict:\n",
        "                        node_i_label = labels_dict.get(i, f\"Node_{i+1}\")\n",
        "                        node_j_label = labels_dict.get(j, f\"Node_{j+1}\")\n",
        "                    else:\n",
        "                        node_i_label = f\"Node_{i+1}\"\n",
        "                        node_j_label = f\"Node_{j+1}\"\n",
        "                    high_sensitivity_pairs.append({\n",
        "                        'Node_i': i + 1,\n",
        "                        'Node_i_Label': node_i_label,\n",
        "                        'Node_j': j + 1,\n",
        "                        'Node_j_Label': node_j_label,\n",
        "                        'Sensitivity': mean_sensitivity[i, j].item()\n",
        "                    })\n",
        "    df_top_pairs = pd.DataFrame(high_sensitivity_pairs)\n",
        "    df_top_pairs = df_top_pairs.sort_values('Sensitivity', key=abs, ascending=False)\n",
        "    df_top_pairs.to_csv(\n",
        "        os.path.join(model_dir, f\"Top5PercentEdges_{'+'.join(edge_feature_modalities)}.csv\"), \n",
        "        index=False\n",
        "    )\n",
        "    # Visualize sensitivity heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sensitivity_numpy = mean_sensitivity.cpu().numpy()\n",
        "    heatmap_params = {\n",
        "        'xticklabels': False,\n",
        "        'yticklabels': False,\n",
        "        'square': True,\n",
        "    }\n",
        "    if style == 'diverging':\n",
        "        heatmap_params.update({\n",
        "            'cmap': 'RdBu_r',\n",
        "            'center': 0,\n",
        "            'vmin': -np.abs(sensitivity_numpy).max(),\n",
        "            'vmax': np.abs(sensitivity_numpy).max()\n",
        "        })\n",
        "    elif style == 'absolute':\n",
        "        heatmap_params.update({\n",
        "            'cmap': 'hot',\n",
        "            'vmin': 0,\n",
        "            'vmax': sensitivity_numpy.max()\n",
        "        })\n",
        "    sns.heatmap(sensitivity_numpy, **heatmap_params)\n",
        "    plt.title(f\"Sensitivity Map: {'+'.join(edge_feature_modalities)}\", fontsize=16, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\n",
        "        os.path.join(model_dir, f\"Sensitivity_{'+'.join(edge_feature_modalities)}.png\"), \n",
        "        dpi=300, \n",
        "        bbox_inches='tight'\n",
        "    )\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    # Print summary with labels\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Network Summary\")\n",
        "    print(\"=\"*70)\n",
        "    # Calculate node-level sensitivity\n",
        "    if directed:\n",
        "        out_sensitivity = mean_sensitivity.sum(dim=1).cpu().numpy()\n",
        "        in_sensitivity = mean_sensitivity.sum(dim=0).cpu().numpy()\n",
        "        node_sensitivity = (out_sensitivity + in_sensitivity) / 2\n",
        "    else:\n",
        "        node_sensitivity = mean_sensitivity.sum(dim=1).cpu().numpy()\n",
        "    print(f\"\\nNumber of nodes: {num_nodes_per_graph}\")\n",
        "    print(f\"Number of top 5% edges: {len(df_top_pairs)}\")\n",
        "    print(f\"Maximum absolute sensitivity: {np.abs(sensitivity_numpy).max():.6f}\")\n",
        "    # Top positive sensitivity nodes\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Nodes with highest positive sensitivity:\")\n",
        "    print(f\"{'='*70}\")\n",
        "    top_positive = np.argsort(node_sensitivity)[-5:][::-1]\n",
        "    for rank, node in enumerate(top_positive, 1):\n",
        "        if labels_dict:\n",
        "            label = labels_dict.get(node, f\"Unknown\")\n",
        "            full_name = full_names_dict.get(node, \"Unknown region\")\n",
        "            print(f\"{rank}. Node {node+1:3d} ({label:15s}): {node_sensitivity[node]:7.3f}\")\n",
        "            print(f\"    {full_name}\")\n",
        "        else:\n",
        "            print(f\"{rank}. Node {node+1:3d}: {node_sensitivity[node]:7.3f}\")\n",
        "    # Top negative sensitivity nodes\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Nodes with lowest negative sensitivity:\")\n",
        "    print(f\"{'='*70}\")\n",
        "    top_negative = np.argsort(node_sensitivity)[:5]\n",
        "    for rank, node in enumerate(top_negative, 1):\n",
        "        if labels_dict:\n",
        "            label = labels_dict.get(node, f\"Unknown\")\n",
        "            full_name = full_names_dict.get(node, \"Unknown region\")\n",
        "            print(f\"{rank}. Node {node+1:3d} ({label:15s}): {node_sensitivity[node]:7.3f}\")\n",
        "            print(f\"    {full_name}\")\n",
        "        else:\n",
        "            print(f\"{rank}. Node {node+1:3d}: {node_sensitivity[node]:7.3f}\")\n",
        "    print(f\"\\n{'='*70}\\n\")\n",
        "\n",
        "def visualize_brain_network(model_dir, atlas_path, edge_feature_modalities, directed=False):\n",
        "    edges_filename = f\"Top5PercentEdges_{'+'.join(edge_feature_modalities)}.csv\"\n",
        "    edges_path = os.path.join(model_dir, edges_filename)\n",
        "    edges_df = pd.read_csv(edges_path)\n",
        "    # Adjust node indices (1-based to 0-based)\n",
        "    edges_df['Node_i'] = edges_df['Node_i'] - 1\n",
        "    edges_df['Node_j'] = edges_df['Node_j'] - 1\n",
        "    # Load atlas and get coordinates\n",
        "    atlas_img = nib.load(atlas_path)\n",
        "    coords = plotting.find_parcellation_cut_coords(atlas_img)\n",
        "    n_nodes = len(coords)\n",
        "    # Build connectivity matrix\n",
        "    connectivity_matrix = np.zeros((n_nodes, n_nodes))\n",
        "    max_abs_sensitivity = np.abs(edges_df['Sensitivity']).max()\n",
        "    edges_df['Scaled_Sensitivity'] = edges_df['Sensitivity'] / max_abs_sensitivity\n",
        "    for _, row in edges_df.iterrows():\n",
        "        i, j = int(row['Node_i']), int(row['Node_j'])\n",
        "        connectivity_matrix[i, j] = row['Scaled_Sensitivity']\n",
        "        if not directed:\n",
        "            connectivity_matrix[j, i] = row['Scaled_Sensitivity']\n",
        "    # Calculate node sensitivity\n",
        "    if directed:\n",
        "        out_sensitivity = np.sum(connectivity_matrix, axis=1)\n",
        "        in_sensitivity = np.sum(connectivity_matrix, axis=0)\n",
        "        node_sensitivity = np.mean([out_sensitivity, in_sensitivity], axis=0)\n",
        "    else:\n",
        "        node_sensitivity = np.sum(connectivity_matrix, axis=1)\n",
        "    max_abs_node_sensitivity = np.abs(node_sensitivity).max()\n",
        "    node_sensitivity = node_sensitivity / max_abs_node_sensitivity\n",
        "    # Create node colors\n",
        "    node_colors = np.zeros((n_nodes, 3))\n",
        "    positive_mask = node_sensitivity > 0\n",
        "    negative_mask = node_sensitivity < 0\n",
        "    node_colors[positive_mask] = np.array([1, 0, 0])\n",
        "    node_colors[negative_mask] = np.array([0, 0, 1])\n",
        "    node_colors *= np.abs(node_sensitivity)[:, np.newaxis]\n",
        "    alpha = 0.3\n",
        "    node_colors = [(plt.cm.colors.to_rgba(c, alpha)) for c in node_colors]\n",
        "    # Plot brain network\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plotting.plot_connectome(\n",
        "        connectivity_matrix, coords,\n",
        "        node_color=node_colors,\n",
        "        node_size=100 * np.abs(node_sensitivity) + 20,\n",
        "        edge_cmap='seismic',\n",
        "        edge_vmin=-1,\n",
        "        edge_vmax=1,\n",
        "        colorbar=True,\n",
        "        edge_threshold=None,\n",
        "        annotate=False,\n",
        "        black_bg=False\n",
        "    )\n",
        "    plt.suptitle(\"Connectional and regional sensitivity\", bbox=dict(facecolor='white', edgecolor=None))\n",
        "    # Add legend\n",
        "    sizes = [0.3, 0.6, 0.9]\n",
        "    legend_elements = [plt.scatter([], [], c='black', s=(100 * s + 20), label=f'Node sensitivity: {s:.1f}') for s in sizes]\n",
        "    plt.legend(handles=legend_elements, loc='best', title='Node size scale', frameon=True, facecolor='white', edgecolor='black', labelcolor='black')\n",
        "    bn_path = os.path.join(model_dir, f\"Top5PercentEdges_{'+'.join(edge_feature_modalities)}_BrainNetwork.png\")\n",
        "    plt.savefig(bn_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def apply_best_model(model_dir, model, device, test_loader, pred_dir, subjects):\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(torch.load(os.path.join(model_dir, \"BestMetricModel.pth\")))\n",
        "    model.eval()\n",
        "    os.makedirs(pred_dir, exist_ok=True)\n",
        "    prob_values = []\n",
        "    pred_values = []\n",
        "    with torch.no_grad():\n",
        "        for batch_data in test_loader:\n",
        "            batch_data = batch_data.to(device)\n",
        "            # Model inference\n",
        "            outputs = model(batch_data)\n",
        "            pred_probs = F.softmax(outputs, dim=1)\n",
        "            pred_labels = torch.argmax(pred_probs, dim=1)\n",
        "            prob_values.extend(pred_probs[:, 1].cpu().numpy())\n",
        "            pred_values.extend(pred_labels.cpu().numpy())\n",
        "    # Save predictions\n",
        "    pred_df = pd.DataFrame({\n",
        "        'ID': subjects,\n",
        "        'Probability': prob_values,\n",
        "        'PredictedSex': pred_values\n",
        "    })\n",
        "    pred_df.to_csv(os.path.join(pred_dir, \"PredictedSex.csv\"), index=False)\n",
        "\n",
        "def calculate_test_metric(metric, gt_dir, pred_dir):\n",
        "    # Load ground truth and predictions\n",
        "    gt_df = pd.read_csv(os.path.join(gt_dir, 'Subjects_GT.csv'))\n",
        "    pred_df = pd.read_csv(os.path.join(pred_dir, 'PredictedSex.csv'))\n",
        "    # Merge on subject ID\n",
        "    merged_df = pd.merge(gt_df, pred_df, on='ID', how='inner')\n",
        "    if len(merged_df) == 0:\n",
        "        print(\"Error: No matching subjects found!\")\n",
        "        return\n",
        "    # Extract values\n",
        "    true_labels = merged_df['Sex'].values\n",
        "    pred_labels = merged_df['PredictedSex'].values\n",
        "    pred_probs = merged_df['Probability'].values\n",
        "    # Compute accuracy\n",
        "    metric.reset()\n",
        "    y_pred = torch.from_numpy(pred_labels).long()\n",
        "    y_true = torch.from_numpy(true_labels).long()\n",
        "    metric(y_pred, y_true)\n",
        "    accuracy = metric.compute().item()\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    # Calculate additional metrics\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall for positive class\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # Recall for negative class\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    f1 = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0 \n",
        "    # Print results\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Classification performance:\")\n",
        "    print(f\"\\u2022 Accuracy: {accuracy:.3f}\")\n",
        "    print(f\"\\u2022 Sensitivity (Recall): {sensitivity:.3f}\")\n",
        "    print(f\"\\u2022 Specificity: {specificity:.3f}\")\n",
        "    print(f\"\\u2022 Precision: {precision:.3f}\")\n",
        "    print(f\"\\u2022 F1-Score: {f1:.3f}\")\n",
        "    print(f\"\\nConfusion matrix:\")\n",
        "    print(f\"\\u2022 True negatives (TN): {tn}\")\n",
        "    print(f\"\\u2022 False positives (FP): {fp}\")\n",
        "    print(f\"\\u2022 False negatives (FN): {fn}\")\n",
        "    print(f\"\\u2022 True positives (TP): {tp}\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "    # Save detailed results\n",
        "    results_df = merged_df[['ID', 'Sex', 'PredictedSex', 'Probability']].copy()\n",
        "    results_df['Correct'] = (results_df['Sex'] == results_df['PredictedSex']).astype(int)\n",
        "    results_df.to_csv(os.path.join(pred_dir, \"TestResults.csv\"), index=False)\n",
        "    # Save summary statistics\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Accuracy': [accuracy],\n",
        "        'Sensitivity': [sensitivity],\n",
        "        'Specificity': [specificity],\n",
        "        'Precision': [precision],\n",
        "        'F1_Score': [f1],\n",
        "        'True_Negatives': [tn],\n",
        "        'False_Positives': [fp],\n",
        "        'False_Negatives': [fn],\n",
        "        'True_Positives': [tp]\n",
        "    })\n",
        "    summary_df.to_csv(os.path.join(pred_dir, \"TestSummary.csv\"), index=False)\n",
        "    # Plot confusion matrix with color mapping for each cell\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    # Create custom colormap for each cell\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    # TN and TP: Blue (correct predictions)\n",
        "    # FP and FN: Red (incorrect predictions)\n",
        "    colors = np.array([[0.2, 0.4, 0.8, cm_normalized[0, 0]], # TN (blue)\n",
        "                       [0.8, 0.2, 0.2, cm_normalized[0, 1]], # FP (red)\n",
        "                       [0.8, 0.2, 0.2, cm_normalized[1, 0]], # FN (red)\n",
        "                       [0.2, 0.4, 0.8, cm_normalized[1, 1]]]) # TP (blue)\n",
        "    colors = colors.reshape(2, 2, 4)\n",
        "    # Plot with custom colors\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            ax.add_patch(plt.Rectangle((j, i), 1, 1, facecolor=colors[i, j], edgecolor='white', linewidth=2))\n",
        "            # Add text annotations\n",
        "            text_color = 'white' if cm_normalized[i, j] > 0.5 else 'black'\n",
        "            ax.text(j + 0.5, i + 0.5, str(cm[i, j]), ha='center', va='center', fontsize=24, fontweight='bold', color=text_color)\n",
        "            # Add percentage\n",
        "            ax.text(j + 0.5, i + 0.7, f'({cm_normalized[i, j]*100:.1f}%)', ha='center', va='center', fontsize=12, color=text_color)\n",
        "    ax.set_xlim(0, 2)\n",
        "    ax.set_ylim(0, 2)\n",
        "    ax.set_xticks([0.5, 1.5])\n",
        "    ax.set_yticks([0.5, 1.5])\n",
        "    ax.set_xticklabels(['Female', 'Male'], fontsize=14)\n",
        "    ax.set_yticklabels(['Female', 'Male'], fontsize=14)\n",
        "    ax.invert_yaxis()\n",
        "    plt.xlabel('Predicted', fontsize=16, fontweight='bold')\n",
        "    plt.ylabel('Actual', fontsize=16, fontweight='bold')\n",
        "    plt.title(f\"Confusion Matrix (Accuracy = {accuracy:.3f})\", fontsize=18, fontweight='bold', pad=20)\n",
        "    # Add cell labels\n",
        "    ax.text(0.5, -0.15, 'TN', ha='center', va='center', fontsize=10, color='blue', fontweight='bold', transform=ax.transData)\n",
        "    ax.text(1.5, -0.15, 'FP', ha='center', va='center', fontsize=10, color='red', fontweight='bold', transform=ax.transData)\n",
        "    ax.text(0.5, 2.15, 'FN', ha='center', va='center', fontsize=10, color='red', fontweight='bold', transform=ax.transData)\n",
        "    ax.text(1.5, 2.15, 'TP', ha='center', va='center', fontsize=10, color='blue', fontweight='bold', transform=ax.transData)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(pred_dir, \"ConfusionMatrix.png\"), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    # Plot ROC curve\n",
        "    fpr, tpr, _ = roc_curve(true_labels, pred_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(pred_dir, \"ROC_Curve.png\"), dpi=300)\n",
        "    plt.show()\n",
        "    # Plot Precision-Recall curve\n",
        "    precision_curve, recall_curve, _ = precision_recall_curve(true_labels, pred_probs)\n",
        "    pr_auc = auc(recall_curve, precision_curve)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot(recall_curve, precision_curve, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
        "    plt.axhline(y=true_labels.sum()/len(true_labels), color='navy', linestyle='--', lw=2, label='Random classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall', fontsize=12)\n",
        "    plt.ylabel('Precision', fontsize=12)\n",
        "    plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower left\", fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(pred_dir, \"PR_Curve.png\"), dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = os.path.join(\"SexClassification\", \"Datasets\")\n",
        "model_dir_prefix = \"SexClassification\"\n",
        "model_name = \"GAT\" # Any supported model name: GCN, GraphSAGE (unimodal edge), GAT, TransformerConv (multimodal edge)\n",
        "node_feature_modalities = [\"PC\", \"GC\"]\n",
        "# If file is (N,): directly used as node feature\n",
        "# If file is (N, N): mean of edges computed for each node\n",
        "edge_feature_modalities = [\"FA\", \"MD\"]  \n",
        "\n",
        "# Training parameters\n",
        "test_size = 0.2\n",
        "scaling = 'global' # Options: 'global', 'per_subject', 'none'\n",
        "num_channels = [64, 64, 128]\n",
        "batch_size = 16\n",
        "max_epochs = 100\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-4\n",
        "val_interval = 1\n",
        "es_patience = 30\n",
        "\n",
        "# Setup output directory and logging\n",
        "node_str = '+'.join(node_feature_modalities) if node_feature_modalities else ''\n",
        "edge_str = '+'.join(edge_feature_modalities) if edge_feature_modalities else ''\n",
        "model_dir = f\"{model_dir_prefix}_{model_name}_N[{node_str}]_E[{edge_str}]\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "log_file = os.path.join(model_dir, \"Training.log\")\n",
        "logging.basicConfig(filename=log_file, level=logging.INFO, format=\"%(message)s\")\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Save configuration\n",
        "config = {\n",
        "    'model_name': model_name,\n",
        "    'node_feature_modalities': node_feature_modalities,\n",
        "    'edge_feature_modalities': edge_feature_modalities,\n",
        "    'scaling': scaling,\n",
        "    'num_channels': num_channels\n",
        "}\n",
        "config_file = os.path.join(model_dir, \"Config.json\")\n",
        "with open(config_file, 'w') as f:\n",
        "    json.dump(config, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_determinism(seed=0)\n",
        "train_ds = BrainConnectivityDataset(\n",
        "    os.path.join(data_dir, \"train\"),\n",
        "    model_name=model_name,\n",
        "    node_feature_modalities=node_feature_modalities,\n",
        "    edge_feature_modalities=edge_feature_modalities,\n",
        "    scaling=scaling\n",
        ")\n",
        "train_loader, val_loader = load_data(\n",
        "    train_ds, batch_size=batch_size, inference=False, test_size=test_size\n",
        ")\n",
        "\n",
        "# Check data shape\n",
        "tr = next(iter(train_loader))\n",
        "num_nodes_per_graph = tr.num_nodes // batch_size\n",
        "num_node_features = tr.x.shape[1]\n",
        "num_edge_features = tr.edge_attr.shape[1]\n",
        "print('\\nData shape for training:')\n",
        "print(f'\\u2022 Node features: ({batch_size} \\u00D7 {tr.x.shape[0] // batch_size}, {tr.x.shape[1]}) \\u00D7 {len(train_loader)}')\n",
        "print(f'\\u2022 Edge features: ({batch_size} \\u00D7 {tr.edge_attr.shape[0] // batch_size}, {tr.edge_attr.shape[1]}) \\u00D7 {len(train_loader)}')\n",
        "print(f'\\u2022 Labels: {tuple(tr.y.shape)} \\u00D7 {len(train_loader)}')\n",
        "vl = next(iter(val_loader))\n",
        "print('\\nData shape for validation:')\n",
        "print(f'\\u2022 Node features: ({batch_size} \\u00D7 {vl.x.shape[0] // batch_size}, {vl.x.shape[1]}) \\u00D7 {len(val_loader)}')\n",
        "print(f'\\u2022 Edge features: ({batch_size} \\u00D7 {vl.edge_attr.shape[0] // batch_size}, {vl.edge_attr.shape[1]}) \\u00D7 {len(val_loader)}')\n",
        "print(f'\\u2022 Labels: {tuple(vl.y.shape)} \\u00D7 {len(val_loader)}')\n",
        "\n",
        "# Visualize data\n",
        "sample = next(iter(train_loader))\n",
        "visualize_conn_matrix(sample, edge_feature_modalities, title=\"Training First Sample\")\n",
        "print(\"Value ranges for connectivity matrices:\")\n",
        "for i, modality in enumerate(edge_feature_modalities):\n",
        "    edge_attr = sample.edge_attr[:, i]\n",
        "    print(f\"\\u2022 {modality}: [{edge_attr.min():.3f}, {edge_attr.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_model(model_name, num_node_features, num_edge_features, num_channels=num_channels)\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "model.to(device)\n",
        "print(f\"\\nSelected model: {model_name}\")\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_params:,}\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "metric = Accuracy(task='binary').to(device)\n",
        "model, epoch_loss_values, epoch_metric_values, metric_values = train_model(\n",
        "    model_dir, model, device, train_loader, val_loader, logger,\n",
        "    criterion, metric, max_epochs, learning_rate, weight_decay, val_interval, es_patience\n",
        ")\n",
        "plot_metric_values(model_dir, epoch_loss_values, epoch_metric_values, metric_values, val_interval)\n",
        "\n",
        "# Visualize outcome\n",
        "sample_indices = [20, 50] # Sex = 0, 1\n",
        "for sample_index in sample_indices:\n",
        "    single_dataset = Subset(train_ds, [sample_index - 1])\n",
        "    single_loader = DataLoader(single_dataset, batch_size=1)\n",
        "    sample = next(iter(single_loader))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        batch_data = sample.to(device)\n",
        "        output = model(batch_data)\n",
        "        pred_prob = torch.softmax(output, dim=1)\n",
        "        pred_label = torch.argmax(pred_prob, dim=1)\n",
        "    actual_label = sample.y.item()\n",
        "    title = (f'Training Sample {sample_index}: '\n",
        "             f'Predicted = {\"Male\" if pred_label.item() == 1 else \"Female\"} '\n",
        "             f'({pred_prob[0, pred_label].item():.2f}), '\n",
        "             f'Actual = {\"Male\" if actual_label == 1 else \"Female\"}')\n",
        "    visualize_conn_matrix(sample, edge_feature_modalities, title=title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Occlusion Sensitivity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_file = os.path.join(data_dir, \"brainnetome_labels.csv\")\n",
        "occlusion_sensitivity_analysis(\n",
        "    model,\n",
        "    val_loader,\n",
        "    model_dir,\n",
        "    edge_feature_modalities,\n",
        "    style='diverging',\n",
        "    directed=False,\n",
        "    num_samples=20,\n",
        "    labels_file=labels_file\n",
        ")\n",
        "atlas_path = os.path.join(data_dir, \"BN_Atlas_246_1mm.nii.gz\")\n",
        "visualize_brain_network(\n",
        "    model_dir, \n",
        "    atlas_path, \n",
        "    edge_feature_modalities,\n",
        "    directed=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if scaling == 'global':\n",
        "    test_ds = BrainConnectivityDataset(\n",
        "        os.path.join(data_dir, \"test\"),\n",
        "        model_name=model_name,\n",
        "        node_feature_modalities=node_feature_modalities,\n",
        "        edge_feature_modalities=edge_feature_modalities,\n",
        "        scaling=scaling,\n",
        "        global_stats=train_ds.global_stats\n",
        "    )\n",
        "else:\n",
        "    test_ds = BrainConnectivityDataset(\n",
        "        os.path.join(data_dir, \"test\"),\n",
        "        model_name=model_name,\n",
        "        node_feature_modalities=node_feature_modalities,\n",
        "        edge_feature_modalities=edge_feature_modalities,\n",
        "        scaling=scaling\n",
        "    )\n",
        "test_loader, subjects = load_data(test_ds, batch_size=1, inference=True)\n",
        "pred_dir = os.path.join(model_dir, \"Prediction\")\n",
        "apply_best_model(model_dir, model, device, test_loader, pred_dir, subjects)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Anonymous",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
